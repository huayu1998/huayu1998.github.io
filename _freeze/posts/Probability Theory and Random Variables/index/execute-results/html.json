{
  "hash": "c46877f036f6932aeb5df1a371f9da0e",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory and Random Variables\"\nauthor: \"Huayu Liang\"\ndate: \"now\"\ncategories: [ML, visualization, probability theory]\nimage: \"probability.png\"\n---\n\nImage from the source: [Analytics Yogi: Maximum Likelihood Estimation: Concepts, Examples](https://vitalflux.com/maximum-likelihood-estimation-concepts-examples/)\n\n# Exploring Probability Theory and Random Variables with Python\n\n## 1. Probability Theory\n\nProbability theory is the branch of mathematics that deals with uncertainty and randomness. In machine learning, it is crucial for making decisions based on uncertain or incomplete information. Let's dive into some key concepts.\n\n### **Example: Coin Toss Simulation**\n\nAs a simple example, let's simulate a coin toss experiment using Python. We'll use the **`random`** module to model the randomness of the outcome.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport random\n\n# Simulate a coin toss\noutcomes = ['Heads', 'Tails']\nresult = random.choice(outcomes)\nprint(f\"The coin landed on: {result}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe coin landed on: Heads\n```\n:::\n:::\n\n\n### **Visualizing a Coin Toss**\n\nTo visualize the outcome of the coin toss experiment, we can create a bar chart that shows the probabilities of getting 'Heads' and 'Tails' over multiple trials.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Simulate multiple coin tosses\ntrials = 10000\ntosses = [random.choice(outcomes) for _ in range(trials)]\n\n# Count the occurrences of 'Heads' and 'Tails'\nhead_count = tosses.count('Heads')\ntail_count = tosses.count('Tails')\n\n# Bar Color\ncolor = (0.2, # redness\n         0.4, # greenness\n         0.2, # blueness\n         0.6 # transparency\n         ) \n\n# Create a bar chart\nplt.bar(outcomes, [head_count, tail_count], color=color)\nplt.xlabel('Outcome')\nplt.ylabel('Frequency')\nplt.title(f'Coin Toss Simulation ({trials} Trials)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=601 height=449}\n:::\n:::\n\n\nThis bar chart visualizes the frequencies of 'Heads' and 'Tails' outcomes over 10000 coin toss trials.\n\n## 2. Random Variables\n\nIn probability theory, a random variable is a variable whose values depend on the outcome of a random experiment. Let's explore a discrete random variable.\n\n### **Example: Dice Roll Simulation**\n\nWe'll simulate the roll of a fair six-sided die and visualize the probability distribution of its outcomes.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Simulate a dice roll\ndie_faces = [1, 2, 3, 4, 5, 6]\nresult = random.choice(die_faces)\nprint(f\"The die rolling result: {result}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe die rolling result: 4\n```\n:::\n:::\n\n\n### **Visualizing a Dice Roll**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Simulate multiple dice rolls\nrolls = [random.choice(die_faces) for _ in range(trials)]\n\n# Count the occurrences of each face\nface_counts = [rolls.count(face) for face in die_faces]\n\n# Bar Color\ncolor = (0.2, # redness\n         0.4, # greenness\n         0.2, # blueness\n         0.6 # transparency\n         ) \n\n# Create a bar chart\nplt.bar(die_faces, face_counts, color=color)\nplt.xlabel('Die Face')\nplt.ylabel('Frequency')\nplt.title(f'Dice Roll Simulation ({trials} Rolls)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=601 height=449}\n:::\n:::\n\n\nThis bar chart shows the probability distribution of a fair six-sided die's outcomes over 1000 rolls.\n\n## 3. Application of Probability Theory in Machine Learning\n\n### 3.1 Data\n\nWe'll need some data by making random values. For the purposes of this notebook we'll use the following synthetic, two-class classification dataset, generated by the `make_classification` method packaged into `sklearn`:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=2, n_informative=2, n_redundant=0)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nimport matplotlib.pyplot as plt\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\nplt.axis('off')\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\n(-4.058631209647063, 4.117395258804836, -4.309480492130218, 5.167561533094499)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=540 height=389}\n:::\n:::\n\n\n### 3.2 Calibration Curves\n\n[Calibration curves](https://scikit-learn.org/stable/modules/calibration.html), also referred to as reliability diagrams, compare how well the probabilistic predictions of a binary classifier are calibrated. It plots the frequency of the positive label (to be more precise, an estimation of the conditional event probability $P(Y=1|predict\\_proba)$ on the y-axis against the predicted probability [predict_proba](https://scikit-learn.org/stable/glossary.html#term-predict_proba) of a model on the x-axis.\n\nAn effective method for evaluating the performance of a classifier's probability predictions on your specific dataset is through the use of a **calibration curve**. The procedure for constructing a calibration curve is outlined as follows:\n\nThe calibration curve functions by organizing the probabilities assigned to the predicted records based on the probabilities reported by the classifier. Subsequently, it categorizes these values into bins and computes two metrics.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n# For binary classification tasks predict_proba returns a matrix containing the first class proba in the first entry,\n# and the second class proba in the second entry. Since there are only two classes one is just 1 - n of the other.\n# The calibration_curve implementation expects just one of these classes in an array, so we index that.\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\n\nfrom sklearn.calibration import calibration_curve\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$LogisticRegression$ Calibration Curve\", fontsize=20); pass\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=943 height=515}\n:::\n:::\n\n\nIn this observation, it's evident that `LogisticRegression` produces probability predictions that closely align with the optimal values. While part of this alignment can be attributed to the simplicity of the dataset, the primary factor is the inherent characteristics of logistic regression. `LogisticRegression` is known for generating highly accurate probability predictions because it optimizes log-odds, which conveniently represents class probability. To elaborate, the cost function that `LogisticRegression` optimizes directly incorporates probability values. Consequently, the algorithm consistently yields unbiased and accurate probability estimates.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=943 height=515}\n:::\n:::\n\n\nHere's an example of an algorithm which does this poorly: `GaussianNB`\n\n### 3.3 Probability Calibration\n\nThe classical, parametric approach to probability calibration is called **Platt scaling**. Below is the logistic regression equation! $A$ and $B$ are scaling parameters, to be determined at fitting time (using some kind of maximum likelihood estimation algorithm), which control how the scaling is applied. They are calculated by applying a maximum likelihood estimation algorithm\n\n$$\nP(Y=1 | x_i) = \\frac{1}{1+exp(A*f(x_i)+B)}\n$$\n\nwhere:\n\n-   $x_i$ is a record of interest\n\n-   $f(x_i)$ is the probability assigned to the record by the classifier\n\nThe plots that follow show recipes for applying these transformations to the above data, and the result that they have on the calibration curves.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Create the example dataset and split it.\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Create an uncorrected classifier.\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Uncalibrated')\n\n# Create a corrected classifier.\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Calibrated (Platt)')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$RandomForestClassifier$ Sample Calibration Curve\", fontsize=20); pass\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=943 height=515}\n:::\n:::\n\n\nIn this example, the `RandomForestClassifier` was initially providing probability scores that were reasonably accurate, making a correction not strictly essential. Nevertheless, it's apparent that implementing the `Platt` transformation potentially contributed to further mitigating bias.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Uncalibrated\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Uncalibrated')\n\n# Calibrated\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='isotonic')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Calibrated (Isotonic)')\n\n# Calibrated, Platt\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='orange', label='Calibrated (Platt)')\n\n\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=943 height=515}\n:::\n:::\n\n\nThis plot is more interesting.\n\nIn the blue we have the calibration curve for the original `GaussianNB` classifier.\n\nDisplayed in yellow is the recalibrated curve generated using a Platt correction. Surprisingly, the Platt correction, in this case, had a detrimental impact rather than being beneficial. Upon examining the original calibration curves for different algorithms, it becomes apparent that `GaussianNB` did not exhibit the same sigmoidal error structure as `SVC` and `RandomForest` in the dataset used for this example. Specifically, GaussianNB showed a tendency to undershoot the high-probability data by too much and undershoot the low probabilities by too little. Platt calibration is effective when biases on both sides are roughly equivalent, and unexpectedly, it did not perform well in this scenario.\n\n## Conclusion\n\nProbability theory and random variables are foundational concepts in machine learning that help us deal with uncertainty and randomness. In this blog post, we explored these concepts through simple simulations of coin tosses and dice rolls in Python as well as providing application of Probability Theory in Machine Learning using Probability Calibration Curves example. Visualizations played a crucial role in understanding the probability distributions of these experiments.\n\nUnderstanding probability theory and random variables is essential for various machine learning algorithms, such as Bayesian networks, decision trees, and more. Experiment with different simulations and visualizations to deepen your grasp of these concepts.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}