{
  "hash": "124020c5041bd285bdefaedbd5c02315",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory and Random Variables\"\nauthor: \"Huayu Liang\"\ndate: \"now\"\ncategories: [ML, visualization, probability theory]\nimage: \"probability.png\"\n---\n\nImage from the source: [Analytics Yogi: Maximum Likelihood Estimation: Concepts, Examples](https://vitalflux.com/maximum-likelihood-estimation-concepts-examples/)\n\n# Exploring Probability Theory and Random Variables with Python\n\n## Probability Theory\n\nProbability theory is the branch of mathematics that deals with uncertainty and randomness. In machine learning, it is crucial for making decisions based on uncertain or incomplete information. Let's dive into some key concepts.\n\n### **Coin Toss Simulation**\n\nAs a simple example, let's simulate a coin toss experiment using Python. We'll use the **`random`** module to model the randomness of the outcome.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport random\n\n# Simulate a coin toss\noutcomes = ['Heads', 'Tails']\nresult = random.choice(outcomes)\nprint(f\"The coin landed on: {result}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe coin landed on: Heads\n```\n:::\n:::\n\n\n### **Visualizing a Coin Toss**\n\nTo visualize the outcome of the coin toss experiment, we can create a bar chart that shows the probabilities of getting 'Heads' and 'Tails' over multiple trials.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Simulate multiple coin tosses\ntrials = 10000\ntosses = [random.choice(outcomes) for _ in range(trials)]\n\n# Count the occurrences of 'Heads' and 'Tails'\nhead_count = tosses.count('Heads')\ntail_count = tosses.count('Tails')\n\n# Create a bar chart\nplt.bar(outcomes, [head_count, tail_count])\nplt.xlabel('Outcome')\nplt.ylabel('Frequency')\nplt.title(f'Coin Toss Simulation ({trials} Trials)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=601 height=449}\n:::\n:::\n\n\nThis bar chart visualizes the frequencies of 'Heads' and 'Tails' outcomes over 10000 coin toss trials.\n\n## Random Variables\n\nIn probability theory, a random variable is a variable whose values depend on the outcome of a random experiment. Let's explore a discrete random variable.\n\n### **Dice Roll Simulation**\n\nWe'll simulate the roll of a fair six-sided die and visualize the probability distribution of its outcomes.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Simulate a dice roll\ndie_faces = [1, 2, 3, 4, 5, 6]\nresult = random.choice(die_faces)\nprint(f\"The die shows: {result}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe die shows: 2\n```\n:::\n:::\n\n\n### **Visualizing a Dice Roll**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Simulate multiple dice rolls\nrolls = [random.choice(die_faces) for _ in range(trials)]\n\n# Count the occurrences of each face\nface_counts = [rolls.count(face) for face in die_faces]\n\n# Create a bar chart\nplt.bar(die_faces, face_counts)\nplt.xlabel('Die Face')\nplt.ylabel('Frequency')\nplt.title(f'Dice Roll Simulation ({trials} Rolls)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=601 height=449}\n:::\n:::\n\n\nThis bar chart shows the probability distribution of a fair six-sided die's outcomes over 1000 rolls.\n\n### Probability Calibration curves\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=2, n_informative=2, n_redundant=0)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nimport matplotlib.pyplot as plt\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\nplt.axis('off')\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```\n(-4.058631209647063, 4.117395258804836, -4.309480492130218, 5.167561533094499)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=540 height=389}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n# For binary classification tasks predict_proba returns a matrix containing the first class proba in the first entry,\n# and the second class proba in the second entry. Since there are only two classes one is just 1 - n of the other.\n# The calibration_curve implementation expects just one of these classes in an array, so we index that.\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\n\nfrom sklearn.calibration import calibration_curve\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$LogisticRegression$ Calibration Curve\", fontsize=20); pass\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=943 height=515}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=943 height=515}\n:::\n:::\n\n\n### Probability calibration\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Create the example dataset and split it.\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Create an uncorrected classifier.\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Uncalibrated')\n\n# Create a corrected classifier.\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Calibrated (Platt)')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$RandomForestClassifier$ Sample Calibration Curve\", fontsize=20); pass\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=943 height=515}\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Uncalibrated\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Uncalibrated')\n\n# Calibrated\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='isotonic')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Calibrated (Isotonic)')\n\n# Calibrated, Platt\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='orange', label='Calibrated (Platt)')\n\n\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=943 height=515}\n:::\n:::\n\n\n## Conclusion\n\nProbability theory and random variables are foundational concepts in machine learning that help us deal with uncertainty and randomness. In this blog post, we explored these concepts through simple simulations of coin tosses and dice rolls in Python. Visualizations played a crucial role in understanding the probability distributions of these experiments.\n\nUnderstanding probability theory and random variables is essential for various machine learning algorithms, such as Bayesian networks, decision trees, and more. Experiment with different simulations and visualizations to deepen your grasp of these concepts.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}