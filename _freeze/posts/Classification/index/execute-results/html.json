{
  "hash": "4eae6408a66ee4c1d5f74c02c3eb28b0",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Huayu Liang\"\ndate: \"now\"\ncategories: [ML, visualization, classification]\nimage: \"classification.png\"\n---\n\nImage from the source: [Analytics Yogi: K-Nearest Neighbors (KNN) Python Examples](https://vitalflux.com/k-nearest-neighbors-explained-with-python-examples/)\n\n# Exploring Classification Algorithms with Python\n\n## What is Classification?\n\nThe Classification algorithm, as a [Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning) technique, is employed to categorize new observations based on the knowledge gained from training data. In the classification process, the program utilizes a provided dataset or observations to learn how to assign new observations to distinct classes or groups, such as 0 or 1, red or blue, yes or no, spam or not spam, and so on. Terms like targets, labels, or categories are used interchangeably to denote these classes. As a supervised learning technique, the Classification algorithm requires labeled input data, encompassing both input and output information. The classification process involves transferring a discrete output function $f(y)$ to an input variable $(x)$.\n\nIn simpler terms, classification serves as a form of pattern recognition, wherein classification algorithms analyze training data to identify similar patterns in new datasets.\n\n## Application of Classification in Machine Learning\n\nVarious classification methods can be employed depending on the characteristics of the dataset under consideration. This variability stems from the extensive nature of the study of classification in statistics. Below lists the top five machine learning algorithms.\n\n## Python Libraries\n\nBefore we dive into classification algorithms, we need to set up our environment. We'll be using the following Python libraries:\n\n-   **`scikit-learn`**: This library provides a wide range of tools for building machine learning models.\n\n-   **`matplotlib`** and **`seaborn`**: These libraries will help us create visualizations.\n\n## Dataset Selection\n\nFor this demonstration, we will use the famous Iris dataset, which contains samples of three different species of iris flowers. The goal is to classify each sample into one of these species.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\nX = data.data\ny = data.target\n```\n:::\n\n\n## Visualizing the Data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=data.target_names[y])\nplt.xlabel(data.feature_names[0])\nplt.ylabel(data.feature_names[1])\nplt.title(\"Iris Dataset: Sepal Length vs Sepal Width\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=589 height=449}\n:::\n:::\n\n\nThis plot shows the separation of iris flowers based on their sepal length and sepal width.\n\n## Classification Algorithms\n\n### 1. Naive Bayes\n\nAs a popular supervised machine learning algorithm, Naïve Bayes classifier is used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This modeling relies on the assumption that, given the class, the features of the input data are conditionally independent, facilitating swift and accurate predictions.\n\nIn statistics, Naïve Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes' theorem. This theorem is based on the probability of a hypothesis, given the data. The Naïve Bayes classifier makes the simplifying assumption that all features in the input data are independent, a condition not always met in practical scenarios. Nevertheless, despite this simplification, the naive Bayes classifier is extensively employed due to its efficiency and commendable performance across various real-world applications.\n\n#### 1.1 Ma**thematical Formulation**\n\nBayes theorem provides a way of computing posterior probability $P(c|x)$ from $P(c)$, $P(x)$ and $P(x|c)$. Look at the equation below\n\n$$\nP(c | x) = \\frac{P(x | c) P(c)}{P(x)}\n$$\n\nWhere,\n\n-   $P(c|x)$ is the posterior probability of *class* (c, *target*) given *predictor* (x, *attributes*).\n\n-   $P(c)$ is the prior probability of *class*.\n\n-   $P(x|c)$ is the likelihood which is the probability of the *predictor* given *class*.\n\n-   $P(x)$ is the prior probability of the *predictor*.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Gaussian Naive Bayes classifier\nnaive_bayes_classifier = GaussianNB()\n\n# Train the classifier\nnaive_bayes_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = naive_bayes_classifier.predict(X_test)\n\n# Evaluate the classifier's performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\n# Display results\nprint(f'Accuracy: {accuracy:.2f}\\n')\nprint('Confusion Matrix:')\nprint(conf_matrix)\nprint('\\nClassification Report:')\nprint(classification_rep)\n\n# Visualize the decision boundary (2D projection for simplicity)\nplt.figure(figsize=(8, 6))\n\n# Plot training points\nfor i, c in zip(range(3), ['red', 'green', 'blue']):\n    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=c, label=f'Class {i}', edgecolors='k')\n\n# Plot testing points\nfor i, c in zip(range(3), ['red', 'green', 'blue']):\n    plt.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], c=c, marker='x', s=150, linewidth=2)\n\nplt.title('Naive Bayes Classifier - Iris Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.00\n\nConfusion Matrix:\n[[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      1.00      1.00         9\n           2       1.00      1.00      1.00        11\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=663 height=523}\n:::\n:::\n\n\n### 2. Logistic Regression\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#importing libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndf= pd.read_csv(\"Iris.csv\")\ndf.drop(\"Id\",axis=1,inplace=True)    #droping id\ndf.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nsns.FacetGrid(df, hue=\"Species\", height=5).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\").add_legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=598 height=471}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n#let Create a pair plot of some columns \nsns.pairplot(df.iloc[:,:],hue='Species')  # graph also  tell us about the the realationship between the two columns\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=1093 height=947}\n:::\n:::\n\n\n### 3. **K-Nearest Neighbors (KNN)**\n\n### 4. Support Vector Machine\n\n### 5. Confusion Matrix\n\n## Conclusion\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}