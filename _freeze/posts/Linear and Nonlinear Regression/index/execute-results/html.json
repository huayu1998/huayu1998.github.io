{
  "hash": "17da368d7f2299380ab693f58770e31c",
  "result": {
    "markdown": "---\ntitle: \"Linear & Nonlinear Regression\"\nauthor: \"Huayu Liang\"\ndate: \"now\"\ncategories: [ML, visualization, linear regressson]\nimage: \"linear.png\"\n---\n\nImage from the source: [Analytics Yogi: Linear Regression Python Examples](https://vitalflux.com/linear-regression-explained-python-sklearn-examples/)\n\n# Exploring Regression with Python\n\nRegression is a vital concept in machine learning that helps us model relationships between variables. In this blog post, we'll explore both linear and nonlinear regression, dive into the mathematical formulas, provide explanations, visualize the results, and work with a synthetic dataset.\n\n## What is Linear Regression?\n\nLinear regression is a straightforward approach for modeling the relationship between a dependent variable $(Y)$ and one or more independent variables $(X)$.\n\n### 1. Formula\n\nThe formula for simple linear regression is:\n\n$$\nY = \\beta_0 + \\beta_1 X\n$$\n\nwhere:\n\n-   $Y$ is the dependent variable.\n\n-   $X$ is the independent variable.\n\n-   $\\beta_0$​ is the intercept.\n\n-   $\\beta_1$​ is the slope.\n\n### 2. Python Implementation\n\nTo get started, let's simulate some data and look at how the predicted values $(Y_e)$ differ from the actual value $(Y)$\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Generate 'random' data\nnp.random.seed(0)\nX = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5\nres = 0.5 * np.random.randn(100)       # Generate 100 residual terms\ny = 2 + 0.3 * X + res                  # Actual values of Y\n\n# Create pandas dataframe to store our X and y values\ndf = pd.DataFrame(\n    {'X': X,\n     'y': y}\n)\n\n# Show the first five rows of our dataframe\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=266}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.910131</td>\n      <td>4.714615</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.500393</td>\n      <td>2.076238</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.946845</td>\n      <td>2.548811</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.102233</td>\n      <td>4.615368</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.168895</td>\n      <td>3.264107</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow, we can have an estimate for `alpha` and `beta`, therefore our model can be written as $(Y_e) = 2.003 + 0.323X$*,*​ and then we can make predictions:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Calculate the mean of X and y\nxmean = np.mean(X)\nymean = np.mean(y)\n\n# Calculate the terms needed for the numerator and denominator of beta\ndf['xycov'] = (df['X'] - xmean) * (df['y'] - ymean)\ndf['xvar'] = (df['X'] - xmean)**2\n\n# Calculate beta and alpha\nbeta = df['xycov'].sum() / df['xvar'].sum()\nalpha = ymean - (beta * xmean)\nprint(f'alpha = {alpha}')\nprint(f'beta = {beta}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nalpha = 2.0031670124623426\nbeta = 0.3229396867092763\n```\n:::\n:::\n\n\nWe can create a plot (shown below) by comparing our predicted values `ypred`with the actual values of `y` to gain a clearer visual insight into our model's performance.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nypred = alpha + beta * X\n\n# Plot regression against actual data\nplt.figure(figsize=(12, 6))\nplt.plot(X, ypred)     # regression line\nplt.plot(X, y, 'ro')   # scatter plot showing actual data\nplt.title('Actual vs Predicted')\nplt.xlabel('X')\nplt.ylabel('y')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=948 height=523}\n:::\n:::\n\n\n### 3. Linear Regression on the Real (advertising) Dataset\n\nBelow is the preview of the \"[advertising.csv](https://www.kaggle.com/search?q=advertising+dataset)\" dataset\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Import and display first five rows of advertising dataset\nadvert = pd.read_csv('advertising.csv')\nadvert.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=269}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TV</th>\n      <th>Radio</th>\n      <th>Newspaper</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>230.1</td>\n      <td>37.8</td>\n      <td>69.2</td>\n      <td>22.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44.5</td>\n      <td>39.3</td>\n      <td>45.1</td>\n      <td>10.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17.2</td>\n      <td>45.9</td>\n      <td>69.3</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>151.5</td>\n      <td>41.3</td>\n      <td>58.5</td>\n      <td>16.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>180.8</td>\n      <td>10.8</td>\n      <td>58.4</td>\n      <td>17.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow we can visualise our regression model by plotting `sales_pred` against the TV advertising costs to find the best fit line:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport statsmodels.formula.api as smf\n\n# Initialise and fit linear regression model using `statsmodels`\nmodel = smf.ols('Sales ~ TV', data=advert)\nmodel = model.fit()\n\n\n# Predict values\nsales_pred = model.predict()\n\n# Plot regression against actual data\nplt.figure(figsize=(12, 6))\nplt.plot(advert['TV'], advert['Sales'], 'o')           # scatter plot showing actual data\nplt.plot(advert['TV'], sales_pred, 'r', linewidth=2)   # regression line\nplt.xlabel('TV Advertising Costs')\nplt.ylabel('Sales')\nplt.title('TV vs Sales')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=957 height=523}\n:::\n:::\n\n\n\\\nWith this model, we can make sales predictions for any given expenditure on TV advertising. For instance, in the scenario of raising TV advertising expenses to \\$500, our prediction indicates that sales would increase to \\~35 units.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nnew_X = 500\nmodel.predict({\"TV\": new_X})\n```\n\n::: {.cell-output .cell-output-display execution_count=271}\n```\n0    34.707207\ndtype: float64\n```\n:::\n:::\n\n\n## What is Non-Linear Regression?\n\nNonlinear regression is used when the relationship between variables is not linear and cannot be accurately represented by a straight line.\n\n### 1. Formula\n\nThe formula for a simple nonlinear regression can vary depending on the chosen model. Let's consider a simple polynomial regression:\n\n$$\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2\n$$\n\nwhere:\n\n-   $Y$ is the dependent variable.\n\n-   $X$ is the independent variable.\n\n-   $\\beta_0$​ is the intercept.\n\n-   $\\beta_1$​ is the coefficient for the linear term.\n\n-   $\\beta_2$​ is the coefficient for the quadratic term.\n\n### 2. Non-Linear Regression on the Real (Position vs. Salaries) Dataset\n\nBelow is the preview of the \"[Position_Salaries.csv](https://github.com/BejaminNaibei/dataset/blob/main/Position_Salaries.csv)\" dataset\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Import the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# get the dataset\ndataset = pd.read_csv('Position_Salaries.csv')\n# View the dataset\ndataset.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=272}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Position</th>\n      <th>Level</th>\n      <th>Salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Business Analyst</td>\n      <td>1</td>\n      <td>45000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Junior Consultant</td>\n      <td>2</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Senior Consultant</td>\n      <td>3</td>\n      <td>60000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Manager</td>\n      <td>4</td>\n      <td>80000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Country Manager</td>\n      <td>5</td>\n      <td>110000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nUnique Level & Salary of the features in the dataset:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# split the data into featutes and target variable seperately\nX_l = dataset.iloc[:, 1:-1].values # features set\ny_p = dataset.iloc[:, -1].values # set of study variable\nprint('Unique Level: ', X_l)\nprint('Unique Salary: ', y_p)\ny_p = y_p.reshape(-1,1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnique Level:  [[ 1]\n [ 2]\n [ 3]\n [ 4]\n [ 5]\n [ 6]\n [ 7]\n [ 8]\n [ 9]\n [10]]\nUnique Salary:  [  45000   50000   60000   80000  110000  150000  200000  300000  500000\n 1000000]\n```\n:::\n:::\n\n\nOur data is prepared for the implementation of our SVR model.\n\nNevertheless, before proceeding, we will initially visualize the data to understand the characteristics of the SVR model that aligns best with it. Let's generate a scatter plot for our two variables.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nStdS_X = StandardScaler()\nStdS_y = StandardScaler()\nX_l = StdS_X.fit_transform(X_l)\ny_p = StdS_y.fit_transform(y_p)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplt.scatter(X_l, y_p, color = 'red') # plotting the training set\nplt.title('Scatter Plot') # adding a tittle to our plot\nplt.xlabel('Levels') # adds a label to the x-axis\nplt.ylabel('Salary') # adds a label to the y-axis\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=600 height=449}\n:::\n:::\n\n\nTo implement our model, the first step involves importing it from scikit-learn and creating an object for it.\n\nGiven that we have specified our data as non-linear, we will employ a kernel known as the Radial Basis Function (RBF) kernel.\n\nOnce the kernel function is declared, we proceed to fit our data onto the object. The subsequent program executes these steps:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# import the model\nfrom sklearn.svm import SVR\n# create the model object\nregressor = SVR(kernel = 'rbf')\n# fit the model on the data\nregressor.fit(X_l, y_p)\n# Make a prediction\nA=regressor.predict(StdS_X.transform([[6.5]]))\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[-0.27861589]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n```\n:::\n:::\n\n\nNow that we have learned how to implement the SVR model and make predictions, the last step is to visualize our model.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# inverse the transformation to go back to the initial scale\nplt.scatter(StdS_X.inverse_transform(X_l), StdS_y.inverse_transform(y_p), color = 'red')\nplt.plot(StdS_X.inverse_transform(X_l), StdS_y.inverse_transform(regressor.predict(X_l).reshape(-1,1)), color = 'blue')\n# add the title to the plot\nplt.title('Support Vector Regression Model')\n# label x axis\nplt.xlabel('Position')\n# label y axis\nplt.ylabel('Salary Level')\n# print the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=589 height=449}\n:::\n:::\n\n\n## Conclusion\n\nIn this blog post, we explored both linear and nonlinear regression in machine learning. We discussed the mathematical formulas, provided Python implementations, and visualized the results using synthetic data. Linear regression is suitable for modeling linear relationships, while nonlinear regression, as demonstrated through polynomial regression, can capture more complex patterns in the data.\n\nUnderstanding these regression techniques is essential for modeling and predicting relationships in various fields, including economics, biology, and engineering.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}