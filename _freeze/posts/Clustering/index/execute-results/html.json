{
  "hash": "93bcd37874f5dd0ae2401b7ffe78368f",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Huayu Liang\"\ndate: \"now\"\ncategories: [ML, visualization, clustering]\nimage: \"clustering.png\"\n---\n\nImage from the source: [Analytics Yogi: When to Use Which Clustering Algorithms?](https://vitalflux.com/when-to-use-which-clustering-algorithms/)\n\n# Exploring Clustering with Python\n\n## What is Clustering?\n\nIn machine learning, clustering is a technique used to group a set of data points into subsets, or clusters, based on the inherent similarities among them. The primary goal of clustering is to partition the data in such a way that points within the same group are more similar to each other than they are to points in other groups.\n\nThe process involves organizing data points into clusters by considering certain features or characteristics, without explicit guidance on what those features should be. Unlike supervised learning, where the algorithm is trained on labeled data with predefined categories, clustering is considered unsupervised learning because it deals with unlabeled data, seeking to uncover hidden patterns or structures.\n\n## Application of Clustering in Machine Learning\n\n### K-Means Clustering\n\nK-Means is a partitioning clustering algorithm that divides data into K clusters. It works by minimizing the sum of squared distances between data points and their respective cluster centers.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for demonstration\nnp.random.seed(0)\nX = np.random.rand(100, 2)\n\n# Fit K-Means clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\n# Visualize data points and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100)\nplt.title('K-Means Clustering')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=571 height=431}\n:::\n:::\n\n\nThis visualization demonstrates K-Means clustering in action, with data points grouped into three clusters, and cluster centers shown in red.\n\n### **Hierarchical Clustering**\n\nHierarchical clustering builds a tree-like hierarchy of clusters. It can be represented as a dendrogram, which shows the relationships between data points and clusters at different levels.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Generate sample data for hierarchical clustering\ndata = np.array([[1, 2], [2, 3], [8, 8], [10, 10]])\n\n# Compute linkage matrix\nZ = linkage(data, 'single')\n\n# Create a dendrogram\nplt.figure(figsize=(10, 5))\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=781 height=434}\n:::\n:::\n\n\nThis dendrogram visualizes the hierarchical clustering of sample data, showing the relationships between data points and clusters.\n\n### DBSCAN labels for the Scatter Plot\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\n\n# Generate synthetic data (you can replace this with your own dataset)\nX, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# Apply DBSCAN clustering\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\n# Plot the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='k', s=40)\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=608 height=449}\n:::\n:::\n\n\n-   We use the **`make_moons`** function from scikit-learn to generate synthetic data with two crescent moon-shaped clusters.\n\n-   DBSCAN is applied with specified parameters (**`eps`** for the maximum distance between two samples and **`min_samples`** for the number of samples in a neighborhood for a point to be considered as a core point).\n\n-   The resulting clusters are visualized using a scatter plot where points belonging to the same cluster share the same color.\n\n## Conclusion\n\nClustering algorithms are essential tools for finding patterns and grouping similar data points in machine learning. In this blog post, we explored three common clustering algorithms, K-Means, Hierarchical Clustering, and DBSCAN with Python implementations and visualizations. These techniques can be applied to various domains, including customer segmentation, image analysis, and more.\n\nTo deepen your understanding, experiment with different datasets and explore additional clustering algorithms like Gaussian Mixture Models, and Agglomerative Clustering. Visualization plays a crucial role in grasping the concepts and results of clustering.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}