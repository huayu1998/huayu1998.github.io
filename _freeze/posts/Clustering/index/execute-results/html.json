{
  "hash": "3dc624a99fd7767ed1df65fa6efd45c7",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Huayu Liang\"\ndate: \"now\"\ncategories: [ML, visualization, clustering]\nimage: \"clustering.png\"\n---\n\nImage from the source: [Analytics Yogi: When to Use Which Clustering Algorithms?](https://vitalflux.com/when-to-use-which-clustering-algorithms/)\n\n# Exploring Clustering with Python\n\n## What is Clustering?\n\nIn machine learning, clustering is a technique used to group a set of data points into subsets, or clusters, based on the inherent similarities among them. The primary goal of clustering is to partition the data in such a way that points within the same group are more similar to each other than they are to points in other groups.\n\nThe process involves organizing data points into clusters by considering certain features or characteristics, without explicit guidance on what those features should be. Unlike supervised learning, where the algorithm is trained on labeled data with predefined categories, clustering is considered unsupervised learning because it deals with unlabeled data, seeking to uncover hidden patterns or structures.\n\n## Application of Clustering in Machine Learning\n\n### 1. K-Means Clustering\n\nK-Means is a partitioning clustering algorithm that divides data into K clusters. It works by minimizing the sum of squared distances between data points and their respective cluster centers.\n\n#### 1.1 How does the K-NN work (detailed step)?\n\nThe [K-NN](https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning) working can be explained on the basis of the below algorithm:\n\n![](KNN_Clustering.png)\n\n-   **Step-1:** Select the number K of the neighbors\n\n-   **Step-2:** Calculate the Euclidean distance of **K number of neighbors**\n\n-   **Step-3:** Take the K nearest neighbors as per the calculated Euclidean distance.\n\n-   **Step-4:** Among these k neighbors, count the number of the data points in each category.\n\n-   **Step-5:** Assign the new data points to that category for which the number of the neighbor is maximum.\n\n-   **Step-6:** Our model is ready.\n\n#### 1.2 K-NN Implementation on Random Created Dataset\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for demonstration\nnp.random.seed(0)\nX = np.random.rand(100, 2)\n\n# Fit K-Means clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\n# Visualize data points and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100)\nplt.title('K-Means Clustering')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=571 height=431}\n:::\n:::\n\n\nThis visualization demonstrates K-Means clustering in action, with data points grouped into three clusters, and cluster centers shown in red.\n\n### **2. Hierarchical Clustering**\n\nHierarchical clustering builds a tree-like hierarchy of clusters. It can be represented as a dendrogram, which shows the relationships between data points and clusters at different levels.\n\n#### 2.1 How does the Hierarchical Clustering work (detailed step)?\n\n[Hierarchical clustering](https://www.learndatasci.com/glossary/hierarchical-clustering/) utilizes a metric of distance or similarity to form new clusters. The steps for Agglomerative clustering can be succinctly outlined as follows:\n\n-   **Step-1**: Compute the proximity matrix using a particular distance metric\n\n-   **Step-2**: Each data point is assigned to a cluster\n\n-   **Step-3**: Merge the clusters based on a metric for the similarity between clusters\n\n-   **Step-4**: Update the distance matrix\n\n-   **Step-5**: Repeat Step 3 and Step 4 until only a single cluster remains\n\n#### 2.2 Hierarchical Clustering Implementation on Random Created Dataset\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Generate sample data for hierarchical clustering\ndata = np.array([[1, 2], [2, 3], [8, 8], [10, 10]])\n\n# Compute linkage matrix\nZ = linkage(data, 'single')\n\n# Create a dendrogram\nplt.figure(figsize=(10, 5))\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=781 height=434}\n:::\n:::\n\n\nThis dendrogram visualizes the hierarchical clustering of sample data, showing the relationships between data points and clusters.\n\n### 3. DBSCAN Clustering\n\n#### 3.1 How does DBSCAN work (detailed step)?\n\n-   **Step-1**: The algorithm proceeds by arbitrarily picking up a point in the dataset (until all points have been visited).\n\n-   **Step-2**: If there are at least 'minPoint' points within a radius of '$Îµ$' to the point then we consider all these points to be part of the same cluster.\n\n-   **Step-3**: The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point\n\n#### 3.2 DBSCAN Implementation on Random Created Dataset\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\n\n# Generate synthetic data (you can replace this with your own dataset)\nX, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# Apply DBSCAN clustering\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\n# Plot the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='k', s=40)\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=608 height=449}\n:::\n:::\n\n\n-   We use the **`make_moons`** function from scikit-learn to generate synthetic data with two crescent moon-shaped clusters.\n\n-   DBSCAN is applied with specified parameters (**`eps`** for the maximum distance between two samples and **`min_samples`** for the number of samples in a neighborhood for a point to be considered as a core point).\n\n-   The resulting clusters are visualized using a scatter plot where points belonging to the same cluster share the same color.\n\n## Conclusion\n\nClustering algorithms are essential tools for finding patterns and grouping similar data points in machine learning. In this blog post, we explored three common clustering algorithms, K-Means, Hierarchical Clustering, and DBSCAN with Python implementations and visualizations. These techniques can be applied to various domains, including customer segmentation, image analysis, and more.\n\nTo deepen your understanding, experiment with different datasets and explore additional clustering algorithms like Gaussian Mixture Models, and Agglomerative Clustering. Visualization plays a crucial role in grasping the concepts and results of clustering.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}