{
  "hash": "83fc491d6f645d98a991687bb1829144",
  "result": {
    "markdown": "---\ntitle: \"Anomaly/Outlier Matrix\"\nauthor: \"Huayu Liang\"\ndate: \"now\"\ncategories: [ML, visualization, matrix]\nimage: \"matrix.png\"\n---\n\nImage from the source: [Outlier Detection and Anomaly Detection with Machine Learning](https://medium.com/@mehulved1503/outlier-detection-and-anomaly-detection-with-machine-learning-caa96b34b7f6)\n\n# Exploring Anomaly/Outlier Detection in Machine Learning with Python\n\n## What is Anomaly Detection?\n\nAnomaly detection, also known as outlier detection, refers to the process of identifying patterns or instances that deviate significantly from the norm in a dataset. Anomalies are observations that do not conform to expected behavior and may indicate unusual events, errors, or outliers in the data. Anomaly detection is applied in various fields such as finance, cybersecurity, manufacturing, and healthcare.\n\nThere are several approaches and algorithms used for anomaly detection:\n\n1.  **Statistical Methods:**\n\n    -   **Z-Score:** This method measures the number of standard deviations a data point is from the mean. Points with a high Z-score are considered anomalies.\n\n    -   **IQR (Interquartile Range):** This method defines a range based on the interquartile range and flags points outside this range as anomalies.\n\n2.  **Machine Learning Algorithms:**\n\n    -   **Isolation Forest:** This algorithm builds an ensemble of isolation trees to isolate anomalies. It works by randomly selecting a feature and then randomly selecting a split value between the minimum and maximum values of the selected feature.\n\n    -   **One-Class SVM (Support Vector Machine):** This algorithm is trained on normal instances and aims to find a hyperplane that separates the normal instances from the outliers.\n\n    -   **Local Outlier Factor (LOF**): is an anomaly detection algorithm that assesses the local density deviation of data points in a dataset. LOF works on the principle that anomalies are often characterized by having a significantly lower local density compared to their neighbors.\n\n    -   **Autoencoders:** These are neural network models that learn to encode input data into a lower-dimensional representation and then decode it back to the original data. Anomalies may have higher reconstruction errors.\n\n3.  **Clustering Methods:**\n\n    -   **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** This algorithm groups together data points that are close to each other and identifies points that are in sparser regions as anomalies.\n\n    -   **K-Means Clustering:** Anomalies may be detected by looking at instances that do not belong to any cluster or are in small clusters.\n\nand so on...\n\n## Comparing Anomaly Detection algorithms for outlier detection on Iris dataset\n\nBelow displays the top five rows of the iris dataset from the Scikit Learn which containing the following features:\n\n-   sepal length (cm)\n\n-   sepal width (cm)\n\n-   petal length (cm)\n\n-   petal width (cm)\n\n-   target\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris \ndf = load_iris(as_frame=True).frame \ndf.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNext, the following provides various examples of anomaly detection algorithms\n\n### **1. Local Outlier Factor (LOF)**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.neighbors import LocalOutlierFactor \n\n# Load the datasets \ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the number of neighbors \nlof = LocalOutlierFactor(n_neighbors=5) \n\n# Fit the model to the data \nlof.fit(X) \n\n# Calculate the outlier scores for each point \nscores = lof.negative_outlier_factor_ \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores > np.percentile(scores, 95)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n\tif i not in outliers: \n\t\tplt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n\telse: \n\t\tplt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)\t\t \nplt.title('Anomly by Local Outlier Factor',fontsize=16)\t\t \nplt.show() \n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=593 height=458}\n:::\n:::\n\n\n### 2. Isolation Forest\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.ensemble import IsolationForest \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = IsolationForest(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores < np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n\tif i not in outliers: \n\t\tplt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n\telse: \n\t\tplt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)\t\t \nplt.title('Anomly by Isolation Forest',fontsize=16)\t\t \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=593 height=458}\n:::\n:::\n\n\n### 3. One-class Support Vector Machines (SVMs):\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn import svm \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the nu parameter \nmodel = svm.OneClassSVM(nu=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores < np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n\tif i not in outliers: \n\t\tplt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n\telse: \n\t\tplt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)\t\t \nplt.title('Anomly by One-class Support Vector Machines',fontsize=16)\t\t \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=593 height=458}\n:::\n:::\n\n\n## Other Example: DBSCAN for Outlier Detection and Marking Outliers\n\nLet's explore another example of using DBSCAN to label outliers and we will also mark them with Matplotlib in a scatter plot:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\n\n# Generate the data\nX, y = make_blobs(n_samples=1000, centers=1, cluster_std=4, random_state=123)\n\n# Define the DBSCAN parameters\neps = 3\nmin_samples = 5\n\n# Create the DBSCAN model\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\n\n# Fit the model to the data\ndbscan.fit(X)\n\n# Get the labels of the data points\nlabels = dbscan.labels_\n\n# Identify the outliers\noutliers = np.where(labels == -1)[0]\n\n# Print the number of outliers\nprint(\"Number of outliers:\", len(outliers))\n\n# Plot the data with the outliers highlighted\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.scatter(X[outliers, 0], X[outliers, 1], c=\"red\", marker=\"x\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of outliers: 1\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=577 height=411}\n:::\n:::\n\n\n## Conclusion\n\nAnomaly detection is a critical task in various fields, including fraud detection, network security, and quality control. You can experiment with different datasets, explore other anomaly detection algorithms like K-Means Clustering, Z-score (statistic), and Autoencoders, and adjust threshold values to adapt to specific use cases.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}