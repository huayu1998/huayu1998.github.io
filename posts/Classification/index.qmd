---
title: "Classification"
author: "Huayu Liang"
date: "now"
categories: [ML, visualization, classification]
image: "classification.png"
---

Image from the source: [Analytics Yogi: K-Nearest Neighbors (KNN) Python Examples](https://vitalflux.com/k-nearest-neighbors-explained-with-python-examples/)

# Exploring Classification Algorithms with Python

## What is Classification?

The Classification algorithm, as a [Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning) technique, is employed to categorize new observations based on the knowledge gained from training data. In the classification process, the program utilizes a provided dataset or observations to learn how to assign new observations to distinct classes or groups, such as 0 or 1, red or blue, yes or no, spam or not spam, and so on. Terms like targets, labels, or categories are used interchangeably to denote these classes. As a supervised learning technique, the Classification algorithm requires labeled input data, encompassing both input and output information. The classification process involves transferring a discrete output function $f(y)$ to an input variable $(x)$.

In simpler terms, classification serves as a form of pattern recognition, wherein classification algorithms analyze training data to identify similar patterns in new datasets.

## Application of Classification in Machine Learning

Various classification methods can be employed depending on the characteristics of the dataset under consideration. This variability stems from the extensive nature of the study of classification in statistics. Below lists the top five machine learning algorithms.

## Python Libraries

Before we dive into classification algorithms, we need to set up our environment. We'll be using the following Python libraries:

-   **`scikit-learn`**: This library provides a wide range of tools for building machine learning models.

-   **`matplotlib`** and **`seaborn`**: These libraries will help us create visualizations.

## Dataset Selection

For this demonstration, we will use the famous Iris dataset, which contains samples of three different species of iris flowers. The goal is to classify each sample into one of these species.

```{python}
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target
```

## Visualizing the Data

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=data.target_names[y])
plt.xlabel(data.feature_names[0])
plt.ylabel(data.feature_names[1])
plt.title("Iris Dataset: Sepal Length vs Sepal Width")
plt.show()
```

This plot shows the separation of iris flowers based on their sepal length and sepal width.

## Classification Algorithms

### 1. Naive Bayes

As a popular supervised machine learning algorithm, Naïve Bayes classifier is used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This modeling relies on the assumption that, given the class, the features of the input data are conditionally independent, facilitating swift and accurate predictions.

In statistics, Naïve Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes' theorem. This theorem is based on the probability of a hypothesis, given the data. The Naïve Bayes classifier makes the simplifying assumption that all features in the input data are independent, a condition not always met in practical scenarios. Nevertheless, despite this simplification, the naive Bayes classifier is extensively employed due to its efficiency and commendable performance across various real-world applications.

#### 1.1 Ma**thematical Formulation**

Bayes theorem provides a way of computing posterior probability $P(c|x)$ from $P(c)$, $P(x)$ and $P(x|c)$. Look at the equation below

$$
P(c | x) = \frac{P(x | c) P(c)}{P(x)}
$$

Where,

-   $P(c|x)$ is the posterior probability of *class* (c, *target*) given *predictor* (x, *attributes*).

-   $P(c)$ is the prior probability of *class*.

-   $P(x|c)$ is the likelihood which is the probability of the *predictor* given *class*.

-   $P(x)$ is the prior probability of the *predictor*.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Gaussian Naive Bayes classifier
naive_bayes_classifier = GaussianNB()

# Train the classifier
naive_bayes_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = naive_bayes_classifier.predict(X_test)

# Evaluate the classifier's performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

# Display results
print(f'Accuracy: {accuracy:.2f}\n')
print('Confusion Matrix:')
print(conf_matrix)
print('\nClassification Report:')
print(classification_rep)

# Visualize the decision boundary (2D projection for simplicity)
plt.figure(figsize=(8, 6))

# Plot training points
for i, c in zip(range(3), ['red', 'green', 'blue']):
    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=c, label=f'Class {i}', edgecolors='k')

# Plot testing points
for i, c in zip(range(3), ['red', 'green', 'blue']):
    plt.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], c=c, marker='x', s=150, linewidth=2)

plt.title('Naive Bayes Classifier - Iris Dataset')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()
```

### 2. Logistic Regression

```{python}
#importing libraries 
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

df= pd.read_csv("Iris.csv")
df.drop("Id",axis=1,inplace=True)    #droping id
df.head(5)
```

```{python}
sns.FacetGrid(df, hue="Species", height=5).map(plt.scatter, "SepalLengthCm", "SepalWidthCm").add_legend()
```

```{python}
#let Create a pair plot of some columns 
sns.pairplot(df.iloc[:,:],hue='Species')  # graph also  tell us about the the realationship between the two columns
```

### 3. **K-Nearest Neighbors (KNN)**

```{python}

#Importing the Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Reading the dataset
loan_dataset = pd.read_csv("train_KNN_DT.csv")
loan_dataset.head()
```

```{python}
loan_dataset.isna().sum()
```

```{python}
loan_dataset['Gender'] = loan_dataset['Gender'].fillna(loan_dataset['Gender'].mode().values[0])
loan_dataset['Married'] = loan_dataset['Married'].fillna(loan_dataset['Married'].mode().values[0])
loan_dataset['Dependents'] = loan_dataset['Dependents'].fillna(loan_dataset['Dependents'].mode().values[0])
loan_dataset['Self_Employed'] = loan_dataset['Self_Employed'].fillna(loan_dataset['Self_Employed'].mode().values[0])
loan_dataset['LoanAmount'] = loan_dataset['LoanAmount'].fillna(loan_dataset['LoanAmount'].mean())
loan_dataset['Loan_Amount_Term'] = loan_dataset['Loan_Amount_Term'].fillna(loan_dataset['Loan_Amount_Term'].mode().values[0] )
loan_dataset['Credit_History'] = loan_dataset['Credit_History'].fillna(loan_dataset['Credit_History'].mode().values[0] )
# Drop the ID column
loan_dataset.drop('Loan_ID', axis=1, inplace=True)
loan_dataset.head()
```

```{python}
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.metrics import accuracy_score
```

```{python}
#Convert some object data type to int
gender = {"Female": 0, "Male": 1}
yes_no = {'No' : 0,'Yes' : 1}
dependents = {'0':0,'1':1,'2':2,'3+':3}
education = {'Not Graduate' : 0, 'Graduate' : 1}
property = {'Semiurban' : 0, 'Urban' : 1,'Rural' : 2}
output = {"N": 0, "Y": 1}

loan_dataset['Gender'] = loan_dataset['Gender'].replace(gender)
loan_dataset['Married'] = loan_dataset['Married'].replace(yes_no)
loan_dataset['Dependents'] = loan_dataset['Dependents'].replace(dependents)
loan_dataset['Education'] = loan_dataset['Education'].replace(education)
loan_dataset['Self_Employed'] = loan_dataset['Self_Employed'].replace(yes_no)
loan_dataset['Property_Area'] = loan_dataset['Property_Area'].replace(property)
loan_dataset['Loan_Status'] = loan_dataset['Loan_Status'].replace(output)

loan_dataset.head()
```

```{python}
# Drop "Loan_Status" and assign it to target variable.
y = loan_dataset.Loan_Status
print(y)
x = loan_dataset.drop('Loan_Status', axis=1, inplace=False)

#Splitting the dataset into train and test set
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state=38, stratify = y)
```

```{python}
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, Y_train)
```

```{python}
prediction_knn = knn.predict(X_test)
print("Prediction for test set: {}".format(prediction_knn))
print('Accuracy of the model: {:.2f}'.format(accuracy_score(Y_test, prediction_knn)*100))
```

```{python}
#Actual value and the predicted value
diff_knn = pd.DataFrame({'Actual value': Y_test, 'Predicted value': prediction_knn})
diff_knn
```

```{python}
#Confusion matrix and classification report
from sklearn import metrics 
from sklearn.metrics import classification_report, confusion_matrix
con_mat = confusion_matrix(Y_test, prediction_knn)
print(con_mat)

sns.heatmap(con_mat, annot=True, fmt="d")
plt.title('Confusion Matrix for KNN')
plt.xlabel('Predicted')
plt.ylabel('True')

print(classification_report(Y_test, prediction_knn))
```

### 4. Decision Tree

```{python}
dTree = tree.DecisionTreeClassifier()
dTree.fit(X_train, Y_train)
```

```{python}
prediction_dt = dTree.predict(X_test)
print("Prediction for test set: {}".format(prediction_dt))
print('Accuracy of the model: {:.2f}'.format(accuracy_score(Y_test, prediction_dt)*100))
```

```{python}
#Actual value and the predicted value
diff_dt = pd.DataFrame({'Actual value': Y_test, 'Predicted value': prediction_dt})
diff_dt
```

```{python}
#Confusion matrix and classification report
con_mat = confusion_matrix(Y_test, prediction_dt)
print(con_mat)

sns.heatmap(con_mat, annot=True, fmt="d")
plt.title('Confusion Matrix for Decision Tree')
plt.xlabel('Predicted')
plt.ylabel('True')

print(classification_report(Y_test, prediction_dt))
```

## Conclusion
