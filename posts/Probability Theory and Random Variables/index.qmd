---
title: "Probability Theory and Random Variables"
author: "Huayu Liang"
date: "now"
categories: [ML, visualization, probability theory]
image: "probability.png"
---

Image from the source: [Analytics Yogi: Maximum Likelihood Estimation: Concepts, Examples](https://vitalflux.com/maximum-likelihood-estimation-concepts-examples/)

# Exploring Probability Theory and Random Variables with Python

## Probability Theory

Probability theory is the branch of mathematics that deals with uncertainty and randomness. In machine learning, it is crucial for making decisions based on uncertain or incomplete information. Let's dive into some key concepts.

### **Coin Toss Simulation**

As a simple example, let's simulate a coin toss experiment using Python. We'll use the **`random`** module to model the randomness of the outcome.

```{python}
import random

# Simulate a coin toss
outcomes = ['Heads', 'Tails']
result = random.choice(outcomes)
print(f"The coin landed on: {result}")
```

### **Visualizing a Coin Toss**

To visualize the outcome of the coin toss experiment, we can create a bar chart that shows the probabilities of getting 'Heads' and 'Tails' over multiple trials.

```{python}
import matplotlib.pyplot as plt

# Simulate multiple coin tosses
trials = 10000
tosses = [random.choice(outcomes) for _ in range(trials)]

# Count the occurrences of 'Heads' and 'Tails'
head_count = tosses.count('Heads')
tail_count = tosses.count('Tails')

# Create a bar chart
plt.bar(outcomes, [head_count, tail_count])
plt.xlabel('Outcome')
plt.ylabel('Frequency')
plt.title(f'Coin Toss Simulation ({trials} Trials)')
plt.show()
```

This bar chart visualizes the frequencies of 'Heads' and 'Tails' outcomes over 10000 coin toss trials.

## Random Variables

In probability theory, a random variable is a variable whose values depend on the outcome of a random experiment. Let's explore a discrete random variable.

### **Dice Roll Simulation**

We'll simulate the roll of a fair six-sided die and visualize the probability distribution of its outcomes.

```{python}
# Simulate a dice roll
die_faces = [1, 2, 3, 4, 5, 6]
result = random.choice(die_faces)
print(f"The die shows: {result}")
```

### **Visualizing a Dice Roll**

```{python}
# Simulate multiple dice rolls
rolls = [random.choice(die_faces) for _ in range(trials)]

# Count the occurrences of each face
face_counts = [rolls.count(face) for face in die_faces]

# Create a bar chart
plt.bar(die_faces, face_counts)
plt.xlabel('Die Face')
plt.ylabel('Frequency')
plt.title(f'Dice Roll Simulation ({trials} Rolls)')
plt.show()
```

This bar chart shows the probability distribution of a fair six-sided die's outcomes over 1000 rolls.

### Probability Calibration curves

```{python}
import numpy as np
from sklearn.datasets import make_classification

np.random.seed(42)
X, y = make_classification(n_samples=100000, n_features=2, n_informative=2, n_redundant=0)
n_train_samples = 1000

X_train, y_train = X[:n_train_samples], y[:n_train_samples]
X_test, y_test = X[n_train_samples:], y[n_train_samples:]

import matplotlib.pyplot as plt
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)
plt.axis('off')
```

```{python}
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(X_train, y_train)

# For binary classification tasks predict_proba returns a matrix containing the first class proba in the first entry,
# and the second class proba in the second entry. Since there are only two classes one is just 1 - n of the other.
# The calibration_curve implementation expects just one of these classes in an array, so we index that.
y_test_predict_proba = clf.predict_proba(X_test)[:, 1]

from sklearn.calibration import calibration_curve
fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)
```

```{python}
import seaborn as sns

fig, ax = plt.subplots(1, figsize=(12, 6))
plt.plot(mean_predicted_value, fraction_of_positives, 's-')
plt.plot([0, 1], [0, 1], '--', color='gray')

sns.despine(left=True, bottom=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.title("$LogisticRegression$ Calibration Curve", fontsize=20); pass
```

```{python}
np.random.seed(42)
X, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)
n_train_samples = 1000

X_train, y_train = X[:n_train_samples], y[:n_train_samples]
X_test, y_test = X[n_train_samples:], y[n_train_samples:]

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, y_train)
y_test_predict_proba = clf.predict_proba(X_test)[:, 1]
fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)

fig, ax = plt.subplots(1, figsize=(12, 6))
plt.plot(mean_predicted_value, fraction_of_positives, 's-')
plt.plot([0, 1], [0, 1], '--', color='gray')

sns.despine(left=True, bottom=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.title("$GaussianNB$ Sample Calibration Curve", fontsize=20); pass
```

### Probability calibration

```{python}
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV

# Create the example dataset and split it.
np.random.seed(42)
X, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)

fig, ax = plt.subplots(1, figsize=(12, 6))

# Create an uncorrected classifier.
clf = RandomForestClassifier()
clf.fit(X_train, y_train)
y_test_predict_proba = clf.predict_proba(X_test)[:, 1]
fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)

plt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Uncalibrated')

# Create a corrected classifier.
clf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')
clf_sigmoid.fit(X_train, y_train)
y_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]
fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)

plt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Calibrated (Platt)')
plt.plot([0, 1], [0, 1], '--', color='gray')

sns.despine(left=True, bottom=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.gca().legend()
plt.title("$RandomForestClassifier$ Sample Calibration Curve", fontsize=20); pass
```

```{python}
X, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)
fig, ax = plt.subplots(1, figsize=(12, 6))

# Uncalibrated
clf = GaussianNB()
clf.fit(X_train, y_train)
y_test_predict_proba = clf.predict_proba(X_test)[:, 1]
fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)

plt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Uncalibrated')

# Calibrated
clf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='isotonic')
clf_sigmoid.fit(X_train, y_train)
y_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]
fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)

plt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Calibrated (Isotonic)')

# Calibrated, Platt
clf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')
clf_sigmoid.fit(X_train, y_train)
y_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]
fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)
plt.plot(mean_predicted_value, fraction_of_positives, 's-', color='orange', label='Calibrated (Platt)')


plt.plot([0, 1], [0, 1], '--', color='gray')

sns.despine(left=True, bottom=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.gca().legend()
plt.title("$GaussianNB$ Sample Calibration Curve", fontsize=20); pass
```

## Conclusion

Probability theory and random variables are foundational concepts in machine learning that help us deal with uncertainty and randomness. In this blog post, we explored these concepts through simple simulations of coin tosses and dice rolls in Python. Visualizations played a crucial role in understanding the probability distributions of these experiments.

Understanding probability theory and random variables is essential for various machine learning algorithms, such as Bayesian networks, decision trees, and more. Experiment with different simulations and visualizations to deepen your grasp of these concepts.
