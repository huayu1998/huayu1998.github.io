---
title: "Linear & Nonlinear Regression"
author: "Huayu Liang"
date: "now"
categories: [ML, visualization, linear regressson]
image: "linear.png"
---

Image from the source: [Analytics Yogi: Linear Regression Python Examples](https://vitalflux.com/linear-regression-explained-python-sklearn-examples/)

# Exploring Regression with Python

Regression is a vital concept in machine learning that helps us model relationships between variables. In this blog post, we'll explore both linear and nonlinear regression, dive into the mathematical formulas, provide explanations, visualize the results, and work with a synthetic dataset.

## What is Linear Regression?

Linear regression is a straightforward approach for modeling the relationship between a dependent variable $(Y)$ and one or more independent variables $(X)$.

### 1. Formula

The formula for simple linear regression is:

$$
Y = \beta_0 + \beta_1 X
$$

where:

-   $Y$ is the dependent variable.

-   $X$ is the independent variable.

-   $\beta_0$​ is the intercept.

-   $\beta_1$​ is the slope.

### 2. Python Implementation

To get started, let's simulate some data and look at how the predicted values $(Y_e)$ differ from the actual value $(Y)$

```{python}
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

# Generate 'random' data
np.random.seed(0)
X = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5
res = 0.5 * np.random.randn(100)       # Generate 100 residual terms
y = 2 + 0.3 * X + res                  # Actual values of Y

# Create pandas dataframe to store our X and y values
df = pd.DataFrame(
    {'X': X,
     'y': y}
)

# Show the first five rows of our dataframe
df.head()
```

Now, we can have an estimate for `alpha` and `beta`, therefore our model can be written as $(Y_e) = 2.003 + 0.323X$*,*​ and then we can make predictions:

```{python}
# Calculate the mean of X and y
xmean = np.mean(X)
ymean = np.mean(y)

# Calculate the terms needed for the numerator and denominator of beta
df['xycov'] = (df['X'] - xmean) * (df['y'] - ymean)
df['xvar'] = (df['X'] - xmean)**2

# Calculate beta and alpha
beta = df['xycov'].sum() / df['xvar'].sum()
alpha = ymean - (beta * xmean)
print(f'alpha = {alpha}')
print(f'beta = {beta}')
```

We can create a plot (shown below) by comparing our predicted values `ypred`with the actual values of `y` to gain a clearer visual insight into our model's performance.

```{python}
ypred = alpha + beta * X

# Plot regression against actual data
plt.figure(figsize=(12, 6))
plt.plot(X, ypred, color='maroon')     # regression line
plt.plot(X, y, 'ro', color='orange')   # scatter plot showing actual data
plt.title('Actual vs Predicted')
plt.xlabel('X')
plt.ylabel('y')

plt.show()
```

### 3. Linear Regression on the Real (advertising) Dataset

Below is the preview of the "[advertising.csv](https://www.kaggle.com/search?q=advertising+dataset)" dataset

```{python}
# Import and display first five rows of advertising dataset
advert = pd.read_csv('advertising.csv')
advert.head(5)
```

Now we can visualise our regression model by plotting `sales_pred` against the TV advertising costs to find the best fit line:

```{python}
import statsmodels.formula.api as smf

# Initialise and fit linear regression model using `statsmodels`
model = smf.ols('Sales ~ TV', data=advert)
model = model.fit()


# Predict values
sales_pred = model.predict()

# Plot regression against actual data
plt.figure(figsize=(12, 6))
plt.plot(advert['TV'], advert['Sales'], 'o', color='orange')           # scatter plot showing actual data
plt.plot(advert['TV'], sales_pred, 'r', linewidth=2, color='maroon')   # regression line
plt.xlabel('TV Advertising Costs')
plt.ylabel('Sales')
plt.title('TV vs Sales')

plt.show()
```

\
With this model, we can make sales predictions for any given expenditure on TV advertising. For instance, in the scenario of raising TV advertising expenses to \$500, our prediction indicates that sales would increase to \~35 units.

```{python}
new_X = 500
model.predict({"TV": new_X})
```

## What is Non-Linear Regression?

Nonlinear regression is used when the relationship between variables is not linear and cannot be accurately represented by a straight line.

### 1. Formula

The formula for a simple nonlinear regression can vary depending on the chosen model. Let's consider a simple polynomial regression:

$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2
$$

where:

-   $Y$ is the dependent variable.

-   $X$ is the independent variable.

-   $\beta_0$​ is the intercept.

-   $\beta_1$​ is the coefficient for the linear term.

-   $\beta_2$​ is the coefficient for the quadratic term.

### 2. Non-Linear Regression on the Real (Position vs. Salaries) Dataset

Below is the preview of the "[Position_Salaries.csv](https://github.com/BejaminNaibei/dataset/blob/main/Position_Salaries.csv)" dataset

```{python}
# Import the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# get the dataset
dataset = pd.read_csv('Position_Salaries.csv')
# View the dataset
dataset.head(5)
```

Unique Level & Salary of the features in the dataset:

```{python}
# split the data into featutes and target variable seperately
X_l = dataset.iloc[:, 1:-1].values # features set
y_p = dataset.iloc[:, -1].values # set of study variable
print('Unique Level: ', X_l)
print('Unique Salary: ', y_p)
y_p = y_p.reshape(-1,1)
```

Our data is prepared for the implementation of our SVR model.

Nevertheless, before proceeding, we will initially visualize the data to understand the characteristics of the SVR model that aligns best with it. Let's generate a scatter plot for our two variables.

```{python}
from sklearn.preprocessing import StandardScaler
StdS_X = StandardScaler()
StdS_y = StandardScaler()
X_l = StdS_X.fit_transform(X_l)
y_p = StdS_y.fit_transform(y_p)
```

```{python}
plt.scatter(X_l, y_p, color = 'maroon') # plotting the training set
plt.title('Scatter Plot') # adding a tittle to our plot
plt.xlabel('Levels') # adds a label to the x-axis
plt.ylabel('Salary') # adds a label to the y-axis
plt.show()
```

To implement our model, the first step involves importing it from scikit-learn and creating an object for it.

Given that we have specified our data as non-linear, we will employ a kernel known as the Radial Basis Function (RBF) kernel.

Once the kernel function is declared, we proceed to fit our data onto the object. The subsequent program executes these steps:

```{python}
# import the model
from sklearn.svm import SVR
# create the model object
regressor = SVR(kernel = 'rbf')
# fit the model on the data
regressor.fit(X_l, y_p)
# Make a prediction
A=regressor.predict(StdS_X.transform([[6.5]]))
print(A)
```

Now that we have learned how to implement the SVR model and make predictions, the last step is to visualize our model.

```{python}
# inverse the transformation to go back to the initial scale
plt.scatter(StdS_X.inverse_transform(X_l), StdS_y.inverse_transform(y_p), color = 'maroon')
plt.plot(StdS_X.inverse_transform(X_l), StdS_y.inverse_transform(regressor.predict(X_l).reshape(-1,1)), color = 'orange')
# add the title to the plot
plt.title('Support Vector Regression Model')
# label x axis
plt.xlabel('Position')
# label y axis
plt.ylabel('Salary Level')
# print the plot
plt.show()
```

## Conclusion

In this blog post, we explored both linear and nonlinear regression in machine learning. We discussed the mathematical formulas, provided Python implementations, and visualized the results using synthetic data. Linear regression is suitable for modeling linear relationships, while nonlinear regression, as demonstrated through polynomial regression, can capture more complex patterns in the data.

Understanding these regression techniques is essential for modeling and predicting relationships in various fields, including economics, biology, and engineering.
