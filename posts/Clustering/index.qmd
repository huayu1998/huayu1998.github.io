---
title: "Clustering"
author: "Huayu Liang"
date: "now"
categories: [ML, visualization, clustering]
image: "clustering.png"
---

Image from the source: [Analytics Yogi: When to Use Which Clustering Algorithms?](https://vitalflux.com/when-to-use-which-clustering-algorithms/)

# Exploring Clustering with Python

## What is Clustering?

In machine learning, clustering is a technique used to group a set of data points into subsets, or clusters, based on the inherent similarities among them. The primary goal of clustering is to partition the data in such a way that points within the same group are more similar to each other than they are to points in other groups.

The process involves organizing data points into clusters by considering certain features or characteristics, without explicit guidance on what those features should be. Unlike supervised learning, where the algorithm is trained on labeled data with predefined categories, clustering is considered unsupervised learning because it deals with unlabeled data, seeking to uncover hidden patterns or structures.

## Application of Clustering in Machine Learning

### 1. K-Means Clustering

K-Means is a partitioning clustering algorithm that divides data into K clusters. It works by minimizing the sum of squared distances between data points and their respective cluster centers.

#### 1.1 How does the K-NN work (detailed step)?

The [K-NN](https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning) working can be explained on the basis of the below algorithm:

![](KNN_Clustering.png)

-   **Step-1:** Select the number K of the neighbors

-   **Step-2:** Calculate the Euclidean distance of **K number of neighbors**

-   **Step-3:** Take the K nearest neighbors as per the calculated Euclidean distance.

-   **Step-4:** Among these k neighbors, count the number of the data points in each category.

-   **Step-5:** Assign the new data points to that category for which the number of the neighbor is maximum.

-   **Step-6:** Our model is ready.

#### 1.2 K-NN Implementation on Random Created Dataset

```{python}
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

# Generate random data for demonstration
np.random.seed(0)
X = np.random.rand(100, 2)

# Fit K-Means clustering model
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)

# Visualize data points and cluster centers
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100)
plt.title('K-Means Clustering')
plt.show()

```

This visualization demonstrates K-Means clustering in action, with data points grouped into three clusters, and cluster centers shown in red.

### **2. Hierarchical Clustering**

Hierarchical clustering builds a tree-like hierarchy of clusters. It can be represented as a dendrogram, which shows the relationships between data points and clusters at different levels.

#### 2.1 How does the Hierarchical Clustering work (detailed step)?

[Hierarchical clustering](https://www.learndatasci.com/glossary/hierarchical-clustering/) utilizes a metric of distance or similarity to form new clusters. The steps for Agglomerative clustering can be succinctly outlined as follows:

-   **Step-1**: Compute the proximity matrix using a particular distance metric

-   **Step-2**: Each data point is assigned to a cluster

-   **Step-3**: Merge the clusters based on a metric for the similarity between clusters

-   **Step-4**: Update the distance matrix

-   **Step-5**: Repeat Step 3 and Step 4 until only a single cluster remains

#### 2.2 Hierarchical Clustering Implementation on Random Created Dataset

```{python}
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Generate sample data for hierarchical clustering
data = np.array([[1, 2], [2, 3], [8, 8], [10, 10]])

# Compute linkage matrix
Z = linkage(data, 'single')

# Create a dendrogram
plt.figure(figsize=(10, 5))
dendrogram(Z)
plt.title('Hierarchical Clustering Dendrogram')
plt.show()
```

This dendrogram visualizes the hierarchical clustering of sample data, showing the relationships between data points and clusters.

### 3. DBSCAN Clustering

#### 3.1 How does DBSCAN work (detailed step)?

-   **Step-1**: The algorithm proceeds by arbitrarily picking up a point in the dataset (until all points have been visited).

-   **Step-2**: If there are at least 'minPoint' points within a radius of '$Îµ$' to the point then we consider all these points to be part of the same cluster.

-   **Step-3**: The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point

#### 3.2 DBSCAN Implementation on Random Created Dataset

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# Generate synthetic data (you can replace this with your own dataset)
X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.3, min_samples=5)
labels = dbscan.fit_predict(X)

# Plot the clustered data
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='k', s=40)
plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
```

-   We use the **`make_moons`** function from scikit-learn to generate synthetic data with two crescent moon-shaped clusters.

-   DBSCAN is applied with specified parameters (**`eps`** for the maximum distance between two samples and **`min_samples`** for the number of samples in a neighborhood for a point to be considered as a core point).

-   The resulting clusters are visualized using a scatter plot where points belonging to the same cluster share the same color.

## Conclusion

Clustering algorithms are essential tools for finding patterns and grouping similar data points in machine learning. In this blog post, we explored three common clustering algorithms, K-Means, Hierarchical Clustering, and DBSCAN with Python implementations and visualizations. These techniques can be applied to various domains, including customer segmentation, image analysis, and more.

To deepen your understanding, experiment with different datasets and explore additional clustering algorithms like Gaussian Mixture Models, and Agglomerative Clustering. Visualization plays a crucial role in grasping the concepts and results of clustering.
