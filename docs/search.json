[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Huayu Liang, a Master’s student in Computer Science focusing specifically on Software Engineering at Virginia Tech. I’m currently working with Dr.Chris Brown in the Code World, No Blanket lab, and I’ve been a GTA for the CS1054 - Intro to Java and am currently TA for the CS5764 - Information Visualization.\n\nFun facts about me: In middle school, I came to the United States for a summer camp as an international student, and that was a great experience in my life. I decided to study abroad in the U.S. for an undergraduate degree to broaden my horizons and study Western culture. I like doing Yoga while free and I love traveling and extreme sports such as bungee jumping, skydiving, and so on. I’d like to make friends and I am a person that is easy to go along with. Besides, I quit sugar for a couple of years to keep healthy for my diet and skin :)\n\n\n Back to top"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html",
    "href": "posts/Probability Theory and Random Variables/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Image from the source: Analytics Yogi: Maximum Likelihood Estimation: Concepts, Examples"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html",
    "href": "posts/Anomaly Outlier Matrix/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Image from the source: Outlier Detection and Anomaly Detection with Machine Learning"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Image from the source: Analytics Yogi: K-Nearest Neighbors (KNN) Python Examples"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Here is some sample R code\n1 + 1\nBut we also want to run Python\n\n\nCode\n```{python}\nfor i in range(9):\n  print(i)\n```\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nCode\n```{python}\nx = 5\n```\n\n\n\n\nCode\n```{python}\n#| output: false\n# Testing a change in Jupyter notebook\n# I'm writing this now in RStudio\n# I'm writing this now in Jupyterlab again\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n```\n\n\nThe plot below is from the Seaborn Python library documentation\n\n\nCode\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"ticks\")\n# Initialize the figure with a logarithmic x axis\nf, ax = plt.subplots(figsize=(7, 6))\nax.set_xscale(\"log\")\n# Load the example planets dataset\nplanets = sns.load_dataset(\"planets\")\n# Plot the orbital period with horizontal boxes\nsns.boxplot(\n    planets, x=\"distance\", y=\"method\", hue=\"method\",\n    whis=[0, 100], width=.6, palette=\"vlag\"\n)\n# Add in points to show each observation\nsns.stripplot(planets, x=\"distance\", y=\"method\", size=4, color=\".3\")\n# Tweak the visual presentation\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\nsns.despine(trim=True, left=True)\n```\n\n\n\n\n\nThe plot below is from the Yellowbrick Python library documentation:\n\n\nCode\n```{python}\nimport numpy as np\nfrom yellowbrick.datasets import load_concrete\nfrom yellowbrick.features import JointPlotVisualizer\n\n# Load the dataset\nX, y = load_concrete()\n\n# Instantiate the visualizer\nvisualizer = JointPlotVisualizer(columns=\"cement\")\n\nvisualizer.fit_transform(X, y)        # Fit and transform the data\nvisualizer.show()                     # Finalize and render the figure\n```\n\n\n\n\n\n&lt;Axes: xlabel='cement', ylabel='target'&gt;\n\n\nThis is a change to the markdown text.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear & Nonlinear Regression",
    "section": "",
    "text": "Image from the source: Analytics Yogi: Linear Regression Python Examples"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Image from the source: Analytics Yogi: When to Use Which Clustering Algorithms?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Huayu Liang",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nanomaly\n\n\noutlier\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nLinear & Nonlinear Regression\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nlinear regressson\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nprobability theory\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nclustering\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Machine Learning\n\n\n\n\n\n\n\nML\n\n\nintroduction\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/Classification/index.html#what-is-classification",
    "href": "posts/Classification/index.html#what-is-classification",
    "title": "Classification",
    "section": "What is Classification?",
    "text": "What is Classification?\nThe Classification algorithm, as a Supervised Learning technique, is employed to categorize new observations based on the knowledge gained from training data. In the classification process, the program utilizes a provided dataset or observations to learn how to assign new observations to distinct classes or groups, such as 0 or 1, red or blue, yes or no, spam or not spam, and so on. Terms like targets, labels, or categories are used interchangeably to denote these classes. As a supervised learning technique, the Classification algorithm requires labeled input data, encompassing both input and output information. The classification process involves transferring a discrete output function \\(f(y)\\) to an input variable \\((x)\\).\nIn simpler terms, classification serves as a form of pattern recognition, wherein classification algorithms analyze training data to identify similar patterns in new datasets."
  },
  {
    "objectID": "posts/Classification/index.html#dateset-selection",
    "href": "posts/Classification/index.html#dateset-selection",
    "title": "Classification",
    "section": "Dateset Selection",
    "text": "Dateset Selection"
  },
  {
    "objectID": "posts/Classification/index.html#visualizing-the-data",
    "href": "posts/Classification/index.html#visualizing-the-data",
    "title": "Classification",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=data.target_names[y])\nplt.xlabel(data.feature_names[0])\nplt.ylabel(data.feature_names[1])\nplt.title(\"Iris Dataset: Sepal Length vs Sepal Width\")\nplt.show()\n\n\n\n\n\nThis plot shows the separation of iris flowers based on their sepal length and sepal width."
  },
  {
    "objectID": "posts/Classification/index.html#classification-algorithms",
    "href": "posts/Classification/index.html#classification-algorithms",
    "title": "Classification",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\n1. Naive Bayes\nAs a popular supervised machine learning algorithm, Naïve Bayes classifier is used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This modeling relies on the assumption that, given the class, the features of the input data are conditionally independent, facilitating swift and accurate predictions.\nIn statistics, Naïve Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes’ theorem. This theorem is based on the probability of a hypothesis, given the data. The Naïve Bayes classifier makes the simplifying assumption that all features in the input data are independent, a condition not always met in practical scenarios. Nevertheless, despite this simplification, the naive Bayes classifier is extensively employed due to its efficiency and commendable performance across various real-world applications.\n\n1.1 Mathematical Formulation\nBayes theorem provides a way of computing posterior probability \\(P(c|x)\\) from \\(P(c)\\), \\(P(x)\\) and \\(P(x|c)\\). Look at the equation below\n\\[\nP(c | x) = \\frac{P(x | c) P(c)}{P(x)}\n\\]\nWhere,\n\n\\(P(c|x)\\) is the posterior probability of class (c, target) given predictor (x, attributes).\n\\(P(c)\\) is the prior probability of class.\n\\(P(x|c)\\) is the likelihood which is the probability of the predictor given class.\n\\(P(x)\\) is the prior probability of the predictor.\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Gaussian Naive Bayes classifier\nnaive_bayes_classifier = GaussianNB()\n\n# Train the classifier\nnaive_bayes_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = naive_bayes_classifier.predict(X_test)\n\n# Evaluate the classifier's performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\n# Display results\nprint(f'Accuracy: {accuracy:.2f}\\n')\nprint('Confusion Matrix:')\nprint(conf_matrix)\nprint('\\nClassification Report:')\nprint(classification_rep)\n\n# Visualize the decision boundary (2D projection for simplicity)\nplt.figure(figsize=(8, 6))\n\n# Plot training points\nfor i, c in zip(range(3), ['red', 'green', 'blue']):\n    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=c, label=f'Class {i}', edgecolors='k')\n\n# Plot testing points\nfor i, c in zip(range(3), ['red', 'green', 'blue']):\n    plt.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], c=c, marker='x', s=150, linewidth=2)\n\nplt.title('Naive Bayes Classifier - Iris Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\nAccuracy: 1.00\n\nConfusion Matrix:\n[[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      1.00      1.00         9\n           2       1.00      1.00      1.00        11\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\n\n\n\n\n\n2. Logistic Regression\n\n\nCode\n#importing libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndf= pd.read_csv(\"Iris.csv\")\ndf.drop(\"Id\",axis=1,inplace=True)    #droping id\ndf.head(5)\n\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(df, hue=\"Species\", height=5).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\").add_legend()\n\n\n\n\n\n\n\nCode\n#let Create a pair plot of some columns \nsns.pairplot(df.iloc[:,:],hue='Species')  # graph also  tell us about the the realationship between the two columns\n\n\n\n\n\n\n\n3. K-Nearest Neighbors (KNN)\n\n\nCode\n#Importing the Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Reading the dataset\nloan_dataset = pd.read_csv(\"train_KNN_DT.csv\")\nloan_dataset.head()\n\n\n\n\n\n\n\n\n\nLoan_ID\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\nLoan_Status\n\n\n\n\n0\nLP001002\nMale\nNo\n0\nGraduate\nNo\n5849\n0.0\nNaN\n360.0\n1.0\nUrban\nY\n\n\n1\nLP001003\nMale\nYes\n1\nGraduate\nNo\n4583\n1508.0\n128.0\n360.0\n1.0\nRural\nN\n\n\n2\nLP001005\nMale\nYes\n0\nGraduate\nYes\n3000\n0.0\n66.0\n360.0\n1.0\nUrban\nY\n\n\n3\nLP001006\nMale\nYes\n0\nNot Graduate\nNo\n2583\n2358.0\n120.0\n360.0\n1.0\nUrban\nY\n\n\n4\nLP001008\nMale\nNo\n0\nGraduate\nNo\n6000\n0.0\n141.0\n360.0\n1.0\nUrban\nY\n\n\n\n\n\n\n\n\n\nCode\nloan_dataset.isna().sum()\n\n\nLoan_ID               0\nGender               13\nMarried               3\nDependents           15\nEducation             0\nSelf_Employed        32\nApplicantIncome       0\nCoapplicantIncome     0\nLoanAmount           22\nLoan_Amount_Term     14\nCredit_History       50\nProperty_Area         0\nLoan_Status           0\ndtype: int64\n\n\n\n\nCode\nloan_dataset['Gender'] = loan_dataset['Gender'].fillna(loan_dataset['Gender'].mode().values[0])\nloan_dataset['Married'] = loan_dataset['Married'].fillna(loan_dataset['Married'].mode().values[0])\nloan_dataset['Dependents'] = loan_dataset['Dependents'].fillna(loan_dataset['Dependents'].mode().values[0])\nloan_dataset['Self_Employed'] = loan_dataset['Self_Employed'].fillna(loan_dataset['Self_Employed'].mode().values[0])\nloan_dataset['LoanAmount'] = loan_dataset['LoanAmount'].fillna(loan_dataset['LoanAmount'].mean())\nloan_dataset['Loan_Amount_Term'] = loan_dataset['Loan_Amount_Term'].fillna(loan_dataset['Loan_Amount_Term'].mode().values[0] )\nloan_dataset['Credit_History'] = loan_dataset['Credit_History'].fillna(loan_dataset['Credit_History'].mode().values[0] )\n# Drop the ID column\nloan_dataset.drop('Loan_ID', axis=1, inplace=True)\nloan_dataset.head()\n\n\n\n\n\n\n\n\n\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\nLoan_Status\n\n\n\n\n0\nMale\nNo\n0\nGraduate\nNo\n5849\n0.0\n146.412162\n360.0\n1.0\nUrban\nY\n\n\n1\nMale\nYes\n1\nGraduate\nNo\n4583\n1508.0\n128.000000\n360.0\n1.0\nRural\nN\n\n\n2\nMale\nYes\n0\nGraduate\nYes\n3000\n0.0\n66.000000\n360.0\n1.0\nUrban\nY\n\n\n3\nMale\nYes\n0\nNot Graduate\nNo\n2583\n2358.0\n120.000000\n360.0\n1.0\nUrban\nY\n\n\n4\nMale\nNo\n0\nGraduate\nNo\n6000\n0.0\n141.000000\n360.0\n1.0\nUrban\nY\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\n\n\n\n\nCode\n#Convert some object data type to int\ngender = {\"Female\": 0, \"Male\": 1}\nyes_no = {'No' : 0,'Yes' : 1}\ndependents = {'0':0,'1':1,'2':2,'3+':3}\neducation = {'Not Graduate' : 0, 'Graduate' : 1}\nproperty = {'Semiurban' : 0, 'Urban' : 1,'Rural' : 2}\noutput = {\"N\": 0, \"Y\": 1}\n\nloan_dataset['Gender'] = loan_dataset['Gender'].replace(gender)\nloan_dataset['Married'] = loan_dataset['Married'].replace(yes_no)\nloan_dataset['Dependents'] = loan_dataset['Dependents'].replace(dependents)\nloan_dataset['Education'] = loan_dataset['Education'].replace(education)\nloan_dataset['Self_Employed'] = loan_dataset['Self_Employed'].replace(yes_no)\nloan_dataset['Property_Area'] = loan_dataset['Property_Area'].replace(property)\nloan_dataset['Loan_Status'] = loan_dataset['Loan_Status'].replace(output)\n\nloan_dataset.head()\n\n\n\n\n\n\n\n\n\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\nLoan_Status\n\n\n\n\n0\n1\n0\n0\n1\n0\n5849\n0.0\n146.412162\n360.0\n1.0\n1\n1\n\n\n1\n1\n1\n1\n1\n0\n4583\n1508.0\n128.000000\n360.0\n1.0\n2\n0\n\n\n2\n1\n1\n0\n1\n1\n3000\n0.0\n66.000000\n360.0\n1.0\n1\n1\n\n\n3\n1\n1\n0\n0\n0\n2583\n2358.0\n120.000000\n360.0\n1.0\n1\n1\n\n\n4\n1\n0\n0\n1\n0\n6000\n0.0\n141.000000\n360.0\n1.0\n1\n1\n\n\n\n\n\n\n\n\n\nCode\n# Drop \"Loan_Status\" and assign it to target variable.\ny = loan_dataset.Loan_Status\nprint(y)\nx = loan_dataset.drop('Loan_Status', axis=1, inplace=False)\n\n#Splitting the dataset into train and test set\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state=38, stratify = y)\n\n\n0      1\n1      0\n2      1\n3      1\n4      1\n      ..\n609    1\n610    1\n611    1\n612    1\n613    0\nName: Loan_Status, Length: 614, dtype: int64\n\n\n\n\nCode\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\n\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\n\n\nCode\nprediction_knn = knn.predict(X_test)\nprint(\"Prediction for test set: {}\".format(prediction_knn))\nprint('Accuracy of the model: {:.2f}'.format(accuracy_score(Y_test, prediction_knn)*100))\n\n\nPrediction for test set: [1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1\n 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n 1 1 1 0 1 1]\nAccuracy of the model: 65.58\n\n\n\n\nCode\n#Actual value and the predicted value\ndiff_knn = pd.DataFrame({'Actual value': Y_test, 'Predicted value': prediction_knn})\ndiff_knn\n\n\n\n\n\n\n\n\n\nActual value\nPredicted value\n\n\n\n\n263\n1\n1\n\n\n395\n1\n1\n\n\n226\n0\n0\n\n\n413\n1\n1\n\n\n403\n1\n0\n\n\n...\n...\n...\n\n\n352\n1\n1\n\n\n238\n1\n1\n\n\n248\n1\n0\n\n\n104\n1\n1\n\n\n8\n1\n1\n\n\n\n\n154 rows × 2 columns\n\n\n\n\n\nCode\n#Confusion matrix and classification report\nfrom sklearn import metrics \nfrom sklearn.metrics import classification_report, confusion_matrix\ncon_mat = confusion_matrix(Y_test, prediction_knn)\nprint(con_mat)\n\nsns.heatmap(con_mat, annot=True, fmt=\"d\")\nplt.title('Confusion Matrix for KNN')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nprint(classification_report(Y_test, prediction_knn))\n\n\n[[ 9 39]\n [14 92]]\n              precision    recall  f1-score   support\n\n           0       0.39      0.19      0.25        48\n           1       0.70      0.87      0.78       106\n\n    accuracy                           0.66       154\n   macro avg       0.55      0.53      0.51       154\nweighted avg       0.61      0.66      0.61       154\n\n\n\n\n\n\n\n\n4. Decision Tree\n\n\nCode\ndTree = tree.DecisionTreeClassifier()\ndTree.fit(X_train, Y_train)\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\n\nCode\nprediction_dt = dTree.predict(X_test)\nprint(\"Prediction for test set: {}\".format(prediction_dt))\nprint('Accuracy of the model: {:.2f}'.format(accuracy_score(Y_test, prediction_dt)*100))\n\n\nPrediction for test set: [1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0\n 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0\n 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1\n 1 1 1 0 1 1]\nAccuracy of the model: 64.94\n\n\n\n\nCode\n#Actual value and the predicted value\ndiff_dt = pd.DataFrame({'Actual value': Y_test, 'Predicted value': prediction_dt})\ndiff_dt\n\n\n\n\n\n\n\n\n\nActual value\nPredicted value\n\n\n\n\n263\n1\n1\n\n\n395\n1\n1\n\n\n226\n0\n1\n\n\n413\n1\n1\n\n\n403\n1\n1\n\n\n...\n...\n...\n\n\n352\n1\n1\n\n\n238\n1\n1\n\n\n248\n1\n0\n\n\n104\n1\n1\n\n\n8\n1\n1\n\n\n\n\n154 rows × 2 columns\n\n\n\n\n\nCode\n#Confusion matrix and classification report\ncon_mat = confusion_matrix(Y_test, prediction_dt)\nprint(con_mat)\n\nsns.heatmap(con_mat, annot=True, fmt=\"d\")\nplt.title('Confusion Matrix for Decision Tree')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nprint(classification_report(Y_test, prediction_dt))\n\n\n[[16 32]\n [22 84]]\n              precision    recall  f1-score   support\n\n           0       0.42      0.33      0.37        48\n           1       0.72      0.79      0.76       106\n\n    accuracy                           0.65       154\n   macro avg       0.57      0.56      0.56       154\nweighted avg       0.63      0.65      0.64       154"
  },
  {
    "objectID": "posts/Classification/index.html#conclusion",
    "href": "posts/Classification/index.html#conclusion",
    "title": "Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored classification algorithms in machine learning, implemented them in Python, and visualized their decision boundaries. We used the Iris and Load datasets as examples and showcased popular classifiers like Naive Bayes, Logistic Regression, K-Nearest Neighbors, and Decision Tree.\nUnderstanding classification algorithms and their performance is crucial for building accurate machine learning models. Python’s extensive libraries and visualization capabilities make it a powerful tool for this task. Experiment with different datasets and classifiers to gain a deeper understanding of classification in machine learning."
  },
  {
    "objectID": "posts/Classification/classification.html",
    "href": "posts/Classification/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Image from the source: Analytics Yogi: K-Nearest Neighbors (KNN) Python Examples"
  },
  {
    "objectID": "posts/Classification/classification.html#what-is-classification",
    "href": "posts/Classification/classification.html#what-is-classification",
    "title": "Classification",
    "section": "What is Classification?",
    "text": "What is Classification?"
  },
  {
    "objectID": "posts/Classification/classification.html#dateset-selection",
    "href": "posts/Classification/classification.html#dateset-selection",
    "title": "Classification",
    "section": "Dateset Selection",
    "text": "Dateset Selection"
  },
  {
    "objectID": "posts/Classification/classification.html#visualizing-the-data",
    "href": "posts/Classification/classification.html#visualizing-the-data",
    "title": "Classification",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data"
  },
  {
    "objectID": "posts/Classification/classification.html#classification-algorithms",
    "href": "posts/Classification/classification.html#classification-algorithms",
    "title": "Classification",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\n1. Naive Bayes\n\n\n2. Logistic Regression\n\n\n3. K-Nearest Neighbors (KNN)\n\n\n4. Confusion Matrix"
  },
  {
    "objectID": "posts/Classification/classification.html#conclusion",
    "href": "posts/Classification/classification.html#conclusion",
    "title": "Classification",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Intro To Machine Learning/index.html",
    "href": "posts/Intro To Machine Learning/index.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Welcome to the Machine Learning World! This blog post covers all the basic concepts of Machine Learning that you’ll need to know as a beginner.\n\nIn today’s world, Machine Learning applications are omnipresent, seamlessly integrating into various aspects of our daily lives. Consider, for instance, the personalized search experience on Google, where Machine Learning algorithms suggest the most relevant results based on your input keywords. Similarly, major platforms like Facebook, YouTube, and Amazon employ recommendation systems to propose products tailored to individual user preferences. Notably, Apple utilizes Machine Learning algorithms for facial and fingerprint recognition, enabling users to unlock their devices without relying on traditional passwords. In essence, Machine Learning has significantly enhanced the convenience and efficiency of our everyday activities.\n\nWhat is Machine Learning?\nIn 1959, Arthur Samuel provided a defining perspective: “Machine Learning is the field of study that gives the computer the ability to learn without explicitly programmed.” A more technical definition emerged in 1997 from Tome Michel: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.”\nIn general terms, Machine Learning (ML) stands as a subset of Artificial Intelligence (AI) with a primary focus on crafting algorithms and models. These models empower computers to learn autonomously and make predictions or decisions without explicit programming. The overarching goal of machine learning is to develop systems capable of learning from data, enhancing their performance over time.\n\n\nWhy Machine Learning?\nTo underscore the significance of Machine Learning, let’s examine the case of spam email recognition. Initially, one must scrutinize the characteristics of spam emails, such as errors, links, and specific keywords like “bank card,” “free,” and “congratulation.” The traditional approach involves analyzing these traits and crafting a program to detect them consistently across all emails. This results in lengthy and intricate code that requires considerable effort for maintenance.\nIn contrast, Machine Learning automates the exploration of spam email characteristics, resulting in significantly shorter and more manageable code. Moreover, machine learning algorithms consistently outperform traditional methods.\nIn summary, several reasons advocate for the adoption of Machine Learning algorithms over traditional methods:\n\nEfficiency: Machine learning algorithms are faster, demand less computational power, and deliver superior results compared to their traditional counterparts.\nProblem-solving Capability: Machine learning techniques can tackle complex problems that traditional methods may find challenging.\nAdaptability: Machine learning systems exhibit adaptability to new data, particularly valuable in fluctuating environments.\nExploration of Complicated Problems: Machine learning algorithms excel in exploring intricate problems with large datasets.\n\nIt’s crucial to note that building a Machine Learning model requires a substantial volume of training data for accurate learning. The model often requires thousands of diverse data points to achieve high accuracy, emphasizing the paramount importance of data in the realm of Machine Learning and Data Science.\n\n\nTypes of Machine Learning Algorithms\nMachine learning algorithms can be broadly classified into three primary types: supervised learning, unsupervised learning, and reinforcement learning. Let’s delve into each category:\n\nSupervised Learning:\nDefinition: In supervised learning, the algorithm undergoes training using a labeled dataset where each input corresponds to a specific output. The objective is for the algorithm to discern the relationship between inputs and outputs, enabling it to make predictions on new, unseen data.\nExamples:\n\nClassification: Predicting discrete labels or categories (e.g., spam or not spam).\n\nRegression: Predicting continuous values (e.g., forecasting house prices).\n\n\n\n\nUnsupervised Learning:\nDefinition: Unsupervised learning entails training the algorithm on an unlabeled dataset. The algorithm’s aim is to discover patterns, structures, or relationships within the data without explicit guidance on what to identify.\n\nExamples:\n\nClustering: Grouping similar data points together (e.g., customer segmentation).\nDimensionality Reduction: Streamlining the number of features while retaining crucial information.\n\n\n\nReinforcement Learning:\nDefinition: Reinforcement learning involves an agent interacting with an environment. The agent takes actions, and the environment responds with feedback in the form of rewards or penalties. The ultimate goal is for the agent to learn optimal strategies that maximize cumulative rewards over time.\nExamples:\n\nGame Playing: Acquiring skills in playing games through rewards for successful moves.\nRobotics: Training robots to execute tasks based on feedback derived from their actions.\n\n\n\n\nUse Cases of Machine Learning\nMachine learning finds practical applications across a wide array of industries, spanning manufacturing, retail, healthcare, life sciences, travel and hospitality, financial services, and energy, feedstock, and utilities. Examples of these applications include:\n\nManufacturing: Utilizing machine learning for predictive maintenance and condition monitoring.\nRetail: Implementing machine learning for upselling and cross-channel marketing strategies.\nHealthcare and Life Sciences: Applying machine learning for disease identification and risk assessment.\nTravel and Hospitality: Employing machine learning for dynamic pricing mechanisms.\nFinancial Services: Utilizing machine learning for risk analytics and regulatory compliance.\nEnergy, Feedstock, and Utilities: Leveraging machine learning for optimizing energy demand and supply.\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#probability-theory",
    "href": "posts/Probability Theory and Random Variables/index.html#probability-theory",
    "title": "Probability Theory and Random Variables",
    "section": "1. Probability Theory",
    "text": "1. Probability Theory\nProbability theory is the branch of mathematics that deals with uncertainty and randomness. In machine learning, it is crucial for making decisions based on uncertain or incomplete information. Let’s dive into some key concepts.\n\nExample: Coin Toss Simulation\nAs a simple example, let’s simulate a coin toss experiment using Python. We’ll use the random module to model the randomness of the outcome.\n\n\nCode\nimport random\n\n# Simulate a coin toss\noutcomes = ['Heads', 'Tails']\nresult = random.choice(outcomes)\nprint(f\"The coin landed on: {result}\")\n\n\nThe coin landed on: Heads\n\n\n\n\nVisualizing a Coin Toss\nTo visualize the outcome of the coin toss experiment, we can create a bar chart that shows the probabilities of getting ‘Heads’ and ‘Tails’ over multiple trials.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Simulate multiple coin tosses\ntrials = 10000\ntosses = [random.choice(outcomes) for _ in range(trials)]\n\n# Count the occurrences of 'Heads' and 'Tails'\nhead_count = tosses.count('Heads')\ntail_count = tosses.count('Tails')\n\n# Bar Color\ncolor = (0.2, # redness\n         0.4, # greenness\n         0.2, # blueness\n         0.6 # transparency\n         ) \n\n# Create a bar chart\nplt.bar(outcomes, [head_count, tail_count], color=color)\nplt.xlabel('Outcome')\nplt.ylabel('Frequency')\nplt.title(f'Coin Toss Simulation ({trials} Trials)')\nplt.show()\n\n\n\n\n\nThis bar chart visualizes the frequencies of ‘Heads’ and ‘Tails’ outcomes over 10000 coin toss trials."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#random-variables",
    "href": "posts/Probability Theory and Random Variables/index.html#random-variables",
    "title": "Probability Theory and Random Variables",
    "section": "2. Random Variables",
    "text": "2. Random Variables\nIn probability theory, a random variable is a variable whose values depend on the outcome of a random experiment. Let’s explore a discrete random variable.\n\nExample: Dice Roll Simulation\nWe’ll simulate the roll of a fair six-sided die and visualize the probability distribution of its outcomes.\n\n\nCode\n# Simulate a dice roll\ndie_faces = [1, 2, 3, 4, 5, 6]\nresult = random.choice(die_faces)\nprint(f\"The die rolling result: {result}\")\n\n\nThe die rolling result: 4\n\n\n\n\nVisualizing a Dice Roll\n\n\nCode\n# Simulate multiple dice rolls\nrolls = [random.choice(die_faces) for _ in range(trials)]\n\n# Count the occurrences of each face\nface_counts = [rolls.count(face) for face in die_faces]\n\n# Bar Color\ncolor = (0.2, # redness\n         0.4, # greenness\n         0.2, # blueness\n         0.6 # transparency\n         ) \n\n# Create a bar chart\nplt.bar(die_faces, face_counts, color=color)\nplt.xlabel('Die Face')\nplt.ylabel('Frequency')\nplt.title(f'Dice Roll Simulation ({trials} Rolls)')\nplt.show()\n\n\n\n\n\nThis bar chart shows the probability distribution of a fair six-sided die’s outcomes over 1000 rolls."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#use-case-in-machine-learning",
    "href": "posts/Probability Theory and Random Variables/index.html#use-case-in-machine-learning",
    "title": "Probability Theory and Random Variables",
    "section": "Use Case in Machine Learning",
    "text": "Use Case in Machine Learning"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#code-example",
    "href": "posts/Probability Theory and Random Variables/index.html#code-example",
    "title": "Probability Theory and Random Variables",
    "section": "Code Example",
    "text": "Code Example"
  },
  {
    "objectID": "posts/Clustering/index.html#what-is-clustering",
    "href": "posts/Clustering/index.html#what-is-clustering",
    "title": "Clustering",
    "section": "What is Clustering?",
    "text": "What is Clustering?\nIn machine learning, clustering is a technique used to group a set of data points into subsets, or clusters, based on the inherent similarities among them. The primary goal of clustering is to partition the data in such a way that points within the same group are more similar to each other than they are to points in other groups.\nThe process involves organizing data points into clusters by considering certain features or characteristics, without explicit guidance on what those features should be. Unlike supervised learning, where the algorithm is trained on labeled data with predefined categories, clustering is considered unsupervised learning because it deals with unlabeled data, seeking to uncover hidden patterns or structures."
  },
  {
    "objectID": "posts/Clustering/index.html#conclusion",
    "href": "posts/Clustering/index.html#conclusion",
    "title": "Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nClustering algorithms are essential tools for finding patterns and grouping similar data points in machine learning. In this blog post, we explored three common clustering algorithms, K-Means, Hierarchical Clustering, and DBSCAN with Python implementations and visualizations. These techniques can be applied to various domains, including customer segmentation, image analysis, and more.\nTo deepen your understanding, experiment with different datasets and explore additional clustering algorithms like Gaussian Mixture Models, and Agglomerative Clustering. Visualization plays a crucial role in grasping the concepts and results of clustering."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#conclusion",
    "href": "posts/Probability Theory and Random Variables/index.html#conclusion",
    "title": "Probability Theory and Random Variables",
    "section": "Conclusion",
    "text": "Conclusion\nProbability theory and random variables are foundational concepts in machine learning that help us deal with uncertainty and randomness. In this blog post, we explored these concepts through simple simulations of coin tosses and dice rolls in Python as well as providing application of Probability Theory in Machine Learning using Probability Calibration Curves example. Visualizations played a crucial role in understanding the probability distributions of these experiments.\nUnderstanding probability theory and random variables is essential for various machine learning algorithms, such as Bayesian networks, decision trees, and more. Experiment with different simulations and visualizations to deepen your grasp of these concepts."
  },
  {
    "objectID": "posts/Clustering/index.html#application-on-of-clustering-in-machine-learning",
    "href": "posts/Clustering/index.html#application-on-of-clustering-in-machine-learning",
    "title": "Clustering",
    "section": "Application on ### of Clustering in Machine Learning",
    "text": "Application on ### of Clustering in Machine Learning"
  },
  {
    "objectID": "posts/Clustering/index.html#application-on-dataset-of-clustering-in-machine-learning",
    "href": "posts/Clustering/index.html#application-on-dataset-of-clustering-in-machine-learning",
    "title": "Clustering",
    "section": "Application on ### Dataset of Clustering in Machine Learning",
    "text": "Application on ### Dataset of Clustering in Machine Learning\n\n\nCode\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for demonstration\nnp.random.seed(0)\nX = np.random.rand(100, 2)\n\n# Fit K-Means clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\n# Visualize data points and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100)\nplt.title('K-Means Clustering')\nplt.show()\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)"
  },
  {
    "objectID": "posts/Clustering/index.html#formula-if-needed",
    "href": "posts/Clustering/index.html#formula-if-needed",
    "title": "Clustering",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#what-is-linear-regression",
    "href": "posts/Linear and Nonlinear Regression/index.html#what-is-linear-regression",
    "title": "Linear & Nonlinear Regression",
    "section": "What is Linear Regression?",
    "text": "What is Linear Regression?\nLinear regression is a straightforward approach for modeling the relationship between a dependent variable \\((Y)\\) and one or more independent variables \\((X)\\).\n\n1. Formula\nThe formula for simple linear regression is:\n\\[\nY = \\beta_0 + \\beta_1 X\n\\]\nwhere:\n\n\\(Y\\) is the dependent variable.\n\\(X\\) is the independent variable.\n\\(\\beta_0\\)​ is the intercept.\n\\(\\beta_1\\)​ is the slope.\n\n\n\n2. Python Implementation\nTo get started, let’s simulate some data and look at how the predicted values \\((Y_e)\\) differ from the actual value \\((Y)\\)\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Generate 'random' data\nnp.random.seed(0)\nX = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5\nres = 0.5 * np.random.randn(100)       # Generate 100 residual terms\ny = 2 + 0.3 * X + res                  # Actual values of Y\n\n# Create pandas dataframe to store our X and y values\ndf = pd.DataFrame(\n    {'X': X,\n     'y': y}\n)\n\n# Show the first five rows of our dataframe\ndf.head()\n\n\n\n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n5.910131\n4.714615\n\n\n1\n2.500393\n2.076238\n\n\n2\n3.946845\n2.548811\n\n\n3\n7.102233\n4.615368\n\n\n4\n6.168895\n3.264107\n\n\n\n\n\n\n\nNow, we can have an estimate for alpha and beta, therefore our model can be written as \\((Y_e) = 2.003 + 0.323X\\),​ and then we can make predictions:\n\n\nCode\n# Calculate the mean of X and y\nxmean = np.mean(X)\nymean = np.mean(y)\n\n# Calculate the terms needed for the numerator and denominator of beta\ndf['xycov'] = (df['X'] - xmean) * (df['y'] - ymean)\ndf['xvar'] = (df['X'] - xmean)**2\n\n# Calculate beta and alpha\nbeta = df['xycov'].sum() / df['xvar'].sum()\nalpha = ymean - (beta * xmean)\nprint(f'alpha = {alpha}')\nprint(f'beta = {beta}')\n\n\nalpha = 2.0031670124623426\nbeta = 0.3229396867092763\n\n\nWe can create a plot (shown below) by comparing our predicted values ypredwith the actual values of y to gain a clearer visual insight into our model’s performance.\n\n\nCode\nypred = alpha + beta * X\n\n# Plot regression against actual data\nplt.figure(figsize=(12, 6))\nplt.plot(X, ypred, color='maroon')     # regression line\nplt.plot(X, y, 'ro', color='orange')   # scatter plot showing actual data\nplt.title('Actual vs Predicted')\nplt.xlabel('X')\nplt.ylabel('y')\n\nplt.show()\n\n\n/var/folders/3x/9fyfnwts50xbsw_b06dnlb3w0000gn/T/ipykernel_6040/3043705735.py:6: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"ro\" (-&gt; color='r'). The keyword argument will take precedence.\n  plt.plot(X, y, 'ro', color='orange')   # scatter plot showing actual data\n\n\n\n\n\n\n\n3. Linear Regression on the Real (advertising) Dataset\nBelow is the preview of the “advertising.csv” dataset\n\n\nCode\n# Import and display first five rows of advertising dataset\nadvert = pd.read_csv('advertising.csv')\nadvert.head(5)\n\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n\n\n\n\n\nNow we can visualise our regression model by plotting sales_pred against the TV advertising costs to find the best fit line:\n\n\nCode\nimport statsmodels.formula.api as smf\n\n# Initialise and fit linear regression model using `statsmodels`\nmodel = smf.ols('Sales ~ TV', data=advert)\nmodel = model.fit()\n\n\n# Predict values\nsales_pred = model.predict()\n\n# Plot regression against actual data\nplt.figure(figsize=(12, 6))\nplt.plot(advert['TV'], advert['Sales'], 'o', color='orange')           # scatter plot showing actual data\nplt.plot(advert['TV'], sales_pred, 'r', linewidth=2, color='maroon')   # regression line\nplt.xlabel('TV Advertising Costs')\nplt.ylabel('Sales')\nplt.title('TV vs Sales')\n\nplt.show()\n\n\n/var/folders/3x/9fyfnwts50xbsw_b06dnlb3w0000gn/T/ipykernel_6040/2225826544.py:14: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"r\" (-&gt; color=(1.0, 0.0, 0.0, 1)). The keyword argument will take precedence.\n  plt.plot(advert['TV'], sales_pred, 'r', linewidth=2, color='maroon')   # regression line\n\n\n\n\n\n\nWith this model, we can make sales predictions for any given expenditure on TV advertising. For instance, in the scenario of raising TV advertising expenses to $500, our prediction indicates that sales would increase to ~35 units.\n\n\nCode\nnew_X = 500\nmodel.predict({\"TV\": new_X})\n\n\n0    34.707207\ndtype: float64"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#what-is-non-linear-regression",
    "href": "posts/Linear and Nonlinear Regression/index.html#what-is-non-linear-regression",
    "title": "Linear & Nonlinear Regression",
    "section": "What is Non-Linear Regression?",
    "text": "What is Non-Linear Regression?\nNonlinear regression is used when the relationship between variables is not linear and cannot be accurately represented by a straight line.\n\n1. Formula\nThe formula for a simple nonlinear regression can vary depending on the chosen model. Let’s consider a simple polynomial regression:\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2\n\\]\nwhere:\n\n\\(Y\\) is the dependent variable.\n\\(X\\) is the independent variable.\n\\(\\beta_0\\)​ is the intercept.\n\\(\\beta_1\\)​ is the coefficient for the linear term.\n\\(\\beta_2\\)​ is the coefficient for the quadratic term.\n\n\n\n2. Non-Linear Regression on the Real (Position vs. Salaries) Dataset\nBelow is the preview of the “Position_Salaries.csv” dataset\n\n\nCode\n# Import the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# get the dataset\ndataset = pd.read_csv('Position_Salaries.csv')\n# View the dataset\ndataset.head(5)\n\n\n\n\n\n\n\n\n\nPosition\nLevel\nSalary\n\n\n\n\n0\nBusiness Analyst\n1\n45000\n\n\n1\nJunior Consultant\n2\n50000\n\n\n2\nSenior Consultant\n3\n60000\n\n\n3\nManager\n4\n80000\n\n\n4\nCountry Manager\n5\n110000\n\n\n\n\n\n\n\nUnique Level & Salary of the features in the dataset:\n\n\nCode\n# split the data into featutes and target variable seperately\nX_l = dataset.iloc[:, 1:-1].values # features set\ny_p = dataset.iloc[:, -1].values # set of study variable\nprint('Unique Level: ', X_l)\nprint('Unique Salary: ', y_p)\ny_p = y_p.reshape(-1,1)\n\n\nUnique Level:  [[ 1]\n [ 2]\n [ 3]\n [ 4]\n [ 5]\n [ 6]\n [ 7]\n [ 8]\n [ 9]\n [10]]\nUnique Salary:  [  45000   50000   60000   80000  110000  150000  200000  300000  500000\n 1000000]\n\n\nOur data is prepared for the implementation of our SVR model.\nNevertheless, before proceeding, we will initially visualize the data to understand the characteristics of the SVR model that aligns best with it. Let’s generate a scatter plot for our two variables.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nStdS_X = StandardScaler()\nStdS_y = StandardScaler()\nX_l = StdS_X.fit_transform(X_l)\ny_p = StdS_y.fit_transform(y_p)\n\n\n\n\nCode\nplt.scatter(X_l, y_p, color = 'maroon') # plotting the training set\nplt.title('Scatter Plot') # adding a tittle to our plot\nplt.xlabel('Levels') # adds a label to the x-axis\nplt.ylabel('Salary') # adds a label to the y-axis\nplt.show()\n\n\n\n\n\nTo implement our model, the first step involves importing it from scikit-learn and creating an object for it.\nGiven that we have specified our data as non-linear, we will employ a kernel known as the Radial Basis Function (RBF) kernel.\nOnce the kernel function is declared, we proceed to fit our data onto the object. The subsequent program executes these steps:\n\n\nCode\n# import the model\nfrom sklearn.svm import SVR\n# create the model object\nregressor = SVR(kernel = 'rbf')\n# fit the model on the data\nregressor.fit(X_l, y_p)\n# Make a prediction\nA=regressor.predict(StdS_X.transform([[6.5]]))\nprint(A)\n\n\n[-0.27861589]\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nNow that we have learned how to implement the SVR model and make predictions, the last step is to visualize our model.\n\n\nCode\n# inverse the transformation to go back to the initial scale\nplt.scatter(StdS_X.inverse_transform(X_l), StdS_y.inverse_transform(y_p), color = 'maroon')\nplt.plot(StdS_X.inverse_transform(X_l), StdS_y.inverse_transform(regressor.predict(X_l).reshape(-1,1)), color = 'orange')\n# add the title to the plot\nplt.title('Support Vector Regression Model')\n# label x axis\nplt.xlabel('Position')\n# label y axis\nplt.ylabel('Salary Level')\n# print the plot\nplt.show()"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#formulas-if-needed",
    "href": "posts/Linear and Nonlinear Regression/index.html#formulas-if-needed",
    "title": "Linear & Nonlinear Regression",
    "section": "Formulas (if needed)",
    "text": "Formulas (if needed)"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#application-on-dataset-of-regression-in-machine-learning",
    "href": "posts/Linear and Nonlinear Regression/index.html#application-on-dataset-of-regression-in-machine-learning",
    "title": "Linear & Nonlinear Regression",
    "section": "Application on ### Dataset of Regression in Machine Learning",
    "text": "Application on ### Dataset of Regression in Machine Learning"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#conslusions",
    "href": "posts/Linear and Nonlinear Regression/index.html#conslusions",
    "title": "Linear & Nonlinear Regression",
    "section": "Conslusions",
    "text": "Conslusions"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#conslusion",
    "href": "posts/Linear and Nonlinear Regression/index.html#conslusion",
    "title": "Linear & Nonlinear Regression",
    "section": "Conslusion",
    "text": "Conslusion"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-anomaly-matrix",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-anomaly-matrix",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Anomaly Matrix?",
    "text": "What is Anomaly Matrix?"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-outlier-matrix",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-outlier-matrix",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Outlier Matrix?",
    "text": "What is Outlier Matrix?"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#application-on-dataset-of-anomalyoutlier-matrix-in-machine-learning",
    "href": "posts/Anomaly Outlier Matrix/index.html#application-on-dataset-of-anomalyoutlier-matrix-in-machine-learning",
    "title": "Anomaly/Outlier Matrix",
    "section": "Application on ### Dataset of Anomaly/Outlier Matrix in Machine Learning",
    "text": "Application on ### Dataset of Anomaly/Outlier Matrix in Machine Learning\n\n\nCode\nfrom sklearn.datasets import load_iris \ndf = load_iris(as_frame=True).frame \ndf.head(5)\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.neighbors import LocalOutlierFactor \n\n# Load the datasets \ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the number of neighbors \nlof = LocalOutlierFactor(n_neighbors=5) \n\n# Fit the model to the data \nlof.fit(X) \n\n# Calculate the outlier scores for each point \nscores = lof.negative_outlier_factor_ \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &gt; np.percentile(scores, 95)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Local Outlier Factor',fontsize=16)      \nplt.show() \n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.ensemble import IsolationForest \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = IsolationForest(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Isolation Forest',fontsize=16)      \nplt.show()\n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.covariance import EllipticEnvelope \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = EllipticEnvelope(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Elliptic Envelope',fontsize=16)         \nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\n\n# Generate the data\nX, y = make_blobs(n_samples=1000, centers=1, cluster_std=4, random_state=123)\n\n# Define the DBSCAN parameters\neps = 3\nmin_samples = 5\n\n# Create the DBSCAN model\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\n\n# Fit the model to the data\ndbscan.fit(X)\n\n# Get the labels of the data points\nlabels = dbscan.labels_\n\n# Identify the outliers\noutliers = np.where(labels == -1)[0]\n\n# Print the number of outliers\nprint(\"Number of outliers:\", len(outliers))\n\n# Plot the data with the outliers highlighted\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.scatter(X[outliers, 0], X[outliers, 1], c=\"red\", marker=\"x\")\nplt.show()\n\n\nNumber of outliers: 1"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#conclusion",
    "href": "posts/Anomaly Outlier Matrix/index.html#conclusion",
    "title": "Anomaly/Outlier Detection",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored anomaly detection in machine learning using different algorithm approaches such as LOF, Isolation Forest, and one-class SVM on the Iris dataset. Beside we also explore the DBSCAN for outlier detection in scatter plot.\nAnomaly detection is a critical task in various fields, including fraud detection, network security, and quality control. You can experiment with different datasets, explore other anomaly detection algorithms like K-Means Clustering, Z-score (statistic), and Autoencoders, and adjust threshold values to adapt to specific use cases."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#formula-if-needed",
    "href": "posts/Probability Theory and Random Variables/index.html#formula-if-needed",
    "title": "Probability Theory and Random Variables",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#formula-if-needed",
    "href": "posts/Linear and Nonlinear Regression/index.html#formula-if-needed",
    "title": "Linear & Nonlinear Regression",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Classification/index.html#formula-if-needed",
    "href": "posts/Classification/index.html#formula-if-needed",
    "title": "Classification",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#formula-if-needed",
    "href": "posts/Anomaly Outlier Matrix/index.html#formula-if-needed",
    "title": "Anomaly/Outlier Matrix",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Classification/index.html#types-of-classification-algorithms",
    "href": "posts/Classification/index.html#types-of-classification-algorithms",
    "title": "Classification",
    "section": "Types of Classification Algorithms",
    "text": "Types of Classification Algorithms\nVarious classification methods can be employed depending on the characteristics of the dataset under consideration. This variability stems from the extensive nature of the study of classification in statistics. Below lists the common four machine learning algorithms.\n\n1. Naive Bayes\nAs a popular supervised machine learning algorithm, Naïve Bayes classifier is used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This modeling relies on the assumption that, given the class, the features of the input data are conditionally independent, facilitating swift and accurate predictions.\nIn statistics, Naïve Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes’ theorem. This theorem is based on the probability of a hypothesis, given the data. The Naïve Bayes classifier makes the simplifying assumption that all features in the input data are independent, a condition not always met in practical scenarios. Nevertheless, despite this simplification, the naive Bayes classifier is extensively employed due to its efficiency and commendable performance across various real-world applications.\n\n1.1 Mathematical Formulation\nBayes theorem provides a way of computing posterior probability \\(P(c|x)\\) from \\(P(c)\\), \\(P(x)\\) and \\(P(x|c)\\). Look at the equation below\n\\[\nP(c | x) = \\frac{P(x | c) P(c)}{P(x)}\n\\]\nWhere,\n\n\\(P(c|x)\\) is the posterior probability of class (c, target) given predictor (x, attributes).\n\\(P(c)\\) is the prior probability of class.\n\\(P(x|c)\\) is the likelihood which is the probability of the predictor given class.\n\\(P(x)\\) is the prior probability of the predictor.\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Gaussian Naive Bayes classifier\nnaive_bayes_classifier = GaussianNB()\n\n# Train the classifier\nnaive_bayes_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = naive_bayes_classifier.predict(X_test)\n\n# Evaluate the classifier's performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\n# Display results\nprint(f'Accuracy: {accuracy:.2f}\\n')\nprint('Confusion Matrix:')\nprint(conf_matrix)\nprint('\\nClassification Report:')\nprint(classification_rep)\n\n# Visualize the decision boundary (2D projection for simplicity)\nplt.figure(figsize=(8, 6))\n\n# Plot training points\nfor i, c in zip(range(3), ['maroon', 'orange', 'magenta']):\n    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=c, label=f'Class {i}', edgecolors='k')\n\n# Plot testing points\nfor i, c in zip(range(3), ['maroon', 'orange', 'magenta']):\n    plt.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], c=c, marker='x', s=150, linewidth=2)\n\nplt.title('Naive Bayes Classifier - Iris Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\nAccuracy: 1.00\n\nConfusion Matrix:\n[[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      1.00      1.00         9\n           2       1.00      1.00      1.00        11\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\n\n\nThis visualization demonstrates Naive Bayes Classifier in action on the Iris Dataset, with data points grouped into three classes.\n\n\n\n2. Logistic Regression\nBelow are the preview of the real-world Iris dataset with the following columns:\n\nId, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species\n\n\n\nCode\n#importing libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndf= pd.read_csv(\"Iris.csv\")\ndf.drop(\"Id\",axis=1,inplace=True)    #droping id\ndf.head(5)\n\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(df, hue=\"Species\", height=5).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\").add_legend()\n\n\n\n\n\nThe plot shows the scatterplot by coloring species\n\n\nCode\n#let Create a pair plot of some columns \nsns.pairplot(df.iloc[:,:],hue='Species')  # graph also  tell us about the the realationship between the two columns\n\n\n\n\n\nThe plot shows the pair plot of the Iris dataset with main column feature by species\n\n\n3. K-Nearest Neighbors (KNN)\nThe aim is to identify the customer segments to whom the loan can be granted using the “loan.csv” dataset, below is the preview of the top five rows of the dataset:\n\n\nCode\n#Importing the Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Reading the dataset\nloan_dataset = pd.read_csv(\"train_KNN_DT.csv\")\nloan_dataset.head()\n\n\n\n\n\n\n\n\n\nLoan_ID\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\nLoan_Status\n\n\n\n\n0\nLP001002\nMale\nNo\n0\nGraduate\nNo\n5849\n0.0\nNaN\n360.0\n1.0\nUrban\nY\n\n\n1\nLP001003\nMale\nYes\n1\nGraduate\nNo\n4583\n1508.0\n128.0\n360.0\n1.0\nRural\nN\n\n\n2\nLP001005\nMale\nYes\n0\nGraduate\nYes\n3000\n0.0\n66.0\n360.0\n1.0\nUrban\nY\n\n\n3\nLP001006\nMale\nYes\n0\nNot Graduate\nNo\n2583\n2358.0\n120.0\n360.0\n1.0\nUrban\nY\n\n\n4\nLP001008\nMale\nNo\n0\nGraduate\nNo\n6000\n0.0\n141.0\n360.0\n1.0\nUrban\nY\n\n\n\n\n\n\n\n\n\nCode\nloan_dataset.isna().sum()\n\n\nLoan_ID               0\nGender               13\nMarried               3\nDependents           15\nEducation             0\nSelf_Employed        32\nApplicantIncome       0\nCoapplicantIncome     0\nLoanAmount           22\nLoan_Amount_Term     14\nCredit_History       50\nProperty_Area         0\nLoan_Status           0\ndtype: int64\n\n\n\n3.1 Dataset Feature Description\n\nLoan_Id: Each applicant is assigned a unique Loan_Id for individual identification purposes.\nGender: This field indicates the gender of the applicant.\nMarried: This field indicates whether the applicant is currently married.\nDependents: This field signifies whether the applicant is financially dependent on someone else.\nEducation: This field provides information about the educational status of the applicant.\nSelf_Employed: This field indicates whether the applicant is self-employed.\nApplicantIncome: This field denotes the income of the applicant.\nCoApplicantIncome: This field represents the income of the co-applicant, where a co-applicant is an individual applying jointly with the borrower for a loan.\nLoan_Amount: This field indicates the amount of the loan that the applicant borrows from the bank.\nLoan_Amount_Term: This field represents the term of the loan amount for each applicant. A term amount loan is characterized by a fixed amount and a predetermined repayment schedule, featuring either a fixed or floating interest rate.\nCredit_History: This field signifies the credit history of each applicant, documenting their responsible repayment of debts.\nLoan_Status: This field indicates whether the loan is approved or not, denoted by ‘Y’ for approved and ‘N’ for not approved.\n\n\n\nCode\nloan_dataset['Gender'] = loan_dataset['Gender'].fillna(loan_dataset['Gender'].mode().values[0])\nloan_dataset['Married'] = loan_dataset['Married'].fillna(loan_dataset['Married'].mode().values[0])\nloan_dataset['Dependents'] = loan_dataset['Dependents'].fillna(loan_dataset['Dependents'].mode().values[0])\nloan_dataset['Self_Employed'] = loan_dataset['Self_Employed'].fillna(loan_dataset['Self_Employed'].mode().values[0])\nloan_dataset['LoanAmount'] = loan_dataset['LoanAmount'].fillna(loan_dataset['LoanAmount'].mean())\nloan_dataset['Loan_Amount_Term'] = loan_dataset['Loan_Amount_Term'].fillna(loan_dataset['Loan_Amount_Term'].mode().values[0] )\nloan_dataset['Credit_History'] = loan_dataset['Credit_History'].fillna(loan_dataset['Credit_History'].mode().values[0] )\n# Drop the ID column\nloan_dataset.drop('Loan_ID', axis=1, inplace=True)\nloan_dataset.head()\n\n\n\n\n\n\n\n\n\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\nLoan_Status\n\n\n\n\n0\nMale\nNo\n0\nGraduate\nNo\n5849\n0.0\n146.412162\n360.0\n1.0\nUrban\nY\n\n\n1\nMale\nYes\n1\nGraduate\nNo\n4583\n1508.0\n128.000000\n360.0\n1.0\nRural\nN\n\n\n2\nMale\nYes\n0\nGraduate\nYes\n3000\n0.0\n66.000000\n360.0\n1.0\nUrban\nY\n\n\n3\nMale\nYes\n0\nNot Graduate\nNo\n2583\n2358.0\n120.000000\n360.0\n1.0\nUrban\nY\n\n\n4\nMale\nNo\n0\nGraduate\nNo\n6000\n0.0\n141.000000\n360.0\n1.0\nUrban\nY\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\n\n\n\n\n3.2 Imputation of the Missing Values\n\n\nCode\n#Convert some object data type to int\ngender = {\"Female\": 0, \"Male\": 1}\nyes_no = {'No' : 0,'Yes' : 1}\ndependents = {'0':0,'1':1,'2':2,'3+':3}\neducation = {'Not Graduate' : 0, 'Graduate' : 1}\nproperty = {'Semiurban' : 0, 'Urban' : 1,'Rural' : 2}\noutput = {\"N\": 0, \"Y\": 1}\n\nloan_dataset['Gender'] = loan_dataset['Gender'].replace(gender)\nloan_dataset['Married'] = loan_dataset['Married'].replace(yes_no)\nloan_dataset['Dependents'] = loan_dataset['Dependents'].replace(dependents)\nloan_dataset['Education'] = loan_dataset['Education'].replace(education)\nloan_dataset['Self_Employed'] = loan_dataset['Self_Employed'].replace(yes_no)\nloan_dataset['Property_Area'] = loan_dataset['Property_Area'].replace(property)\nloan_dataset['Loan_Status'] = loan_dataset['Loan_Status'].replace(output)\n\nloan_dataset.head()\n\n\n\n\n\n\n\n\n\nGender\nMarried\nDependents\nEducation\nSelf_Employed\nApplicantIncome\nCoapplicantIncome\nLoanAmount\nLoan_Amount_Term\nCredit_History\nProperty_Area\nLoan_Status\n\n\n\n\n0\n1\n0\n0\n1\n0\n5849\n0.0\n146.412162\n360.0\n1.0\n1\n1\n\n\n1\n1\n1\n1\n1\n0\n4583\n1508.0\n128.000000\n360.0\n1.0\n2\n0\n\n\n2\n1\n1\n0\n1\n1\n3000\n0.0\n66.000000\n360.0\n1.0\n1\n1\n\n\n3\n1\n1\n0\n0\n0\n2583\n2358.0\n120.000000\n360.0\n1.0\n1\n1\n\n\n4\n1\n0\n0\n1\n0\n6000\n0.0\n141.000000\n360.0\n1.0\n1\n1\n\n\n\n\n\n\n\n\n\nCode\n# Drop \"Loan_Status\" and assign it to target variable.\ny = loan_dataset.Loan_Status\nprint(y)\nx = loan_dataset.drop('Loan_Status', axis=1, inplace=False)\n\n#Splitting the dataset into train and test set\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state=38, stratify = y)\n\n\n0      1\n1      0\n2      1\n3      1\n4      1\n      ..\n609    1\n610    1\n611    1\n612    1\n613    0\nName: Loan_Status, Length: 614, dtype: int64\n\n\n\n\n3.3 K Neighbors Classifier implementation\n\n\nCode\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\n\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\n\n3.3.1 Prediction on the Test Set\n\n\nCode\nprediction_knn = knn.predict(X_test)\nprint(\"Prediction for test set: {}\".format(prediction_knn))\nprint('Accuracy of the model: {:.2f}'.format(accuracy_score(Y_test, prediction_knn)*100))\n\n\nPrediction for test set: [1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1\n 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n 1 1 1 0 1 1]\nAccuracy of the model: 65.58\n\n\n\n\n3.3.2 Actual values and the predicted values\n\n\nCode\n#Actual value and the predicted value\ndiff_knn = pd.DataFrame({'Actual value': Y_test, 'Predicted value': prediction_knn})\ndiff_knn\n\n\n\n\n\n\n\n\n\nActual value\nPredicted value\n\n\n\n\n263\n1\n1\n\n\n395\n1\n1\n\n\n226\n0\n0\n\n\n413\n1\n1\n\n\n403\n1\n0\n\n\n...\n...\n...\n\n\n352\n1\n1\n\n\n238\n1\n1\n\n\n248\n1\n0\n\n\n104\n1\n1\n\n\n8\n1\n1\n\n\n\n\n154 rows × 2 columns\n\n\n\n\n\n3.3.3 Evaluating the K Neighbors Classifier Model\n\n\nCode\n#Confusion matrix and classification report\nfrom sklearn import metrics \nfrom sklearn.metrics import classification_report, confusion_matrix\ncon_mat = confusion_matrix(Y_test, prediction_knn)\nprint(con_mat)\n\nsns.heatmap(con_mat, annot=True, fmt=\"d\", cmap='YlGnBu')\nplt.title('Confusion Matrix for KNN')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nprint(classification_report(Y_test, prediction_knn))\n\n\n[[ 9 39]\n [14 92]]\n              precision    recall  f1-score   support\n\n           0       0.39      0.19      0.25        48\n           1       0.70      0.87      0.78       106\n\n    accuracy                           0.66       154\n   macro avg       0.55      0.53      0.51       154\nweighted avg       0.61      0.66      0.61       154\n\n\n\n\n\n\n\n\n\n\n4. Decision Tree\n\n\n(implementation on the same dataset as the K-Neighbors Classifier)\n\n\nCode\ndTree = tree.DecisionTreeClassifier()\ndTree.fit(X_train, Y_train)\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\n\nCode\nprediction_dt = dTree.predict(X_test)\nprint(\"Prediction for test set: {}\".format(prediction_dt))\nprint('Accuracy of the model: {:.2f}'.format(accuracy_score(Y_test, prediction_dt)*100))\n\n\nPrediction for test set: [1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0\n 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0\n 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1\n 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1\n 1 1 1 0 1 1]\nAccuracy of the model: 66.88\n\n\n\n\nCode\n#Actual value and the predicted value\ndiff_dt = pd.DataFrame({'Actual value': Y_test, 'Predicted value': prediction_dt})\ndiff_dt\n\n\n\n\n\n\n\n\n\nActual value\nPredicted value\n\n\n\n\n263\n1\n1\n\n\n395\n1\n1\n\n\n226\n0\n1\n\n\n413\n1\n1\n\n\n403\n1\n1\n\n\n...\n...\n...\n\n\n352\n1\n1\n\n\n238\n1\n1\n\n\n248\n1\n0\n\n\n104\n1\n1\n\n\n8\n1\n1\n\n\n\n\n154 rows × 2 columns\n\n\n\n\n\nCode\n#Confusion matrix and classification report\ncon_mat = confusion_matrix(Y_test, prediction_dt)\nprint(con_mat)\n\nsns.heatmap(con_mat, annot=True, fmt=\"d\", cmap='YlGnBu')\nplt.title('Confusion Matrix for Decision Tree')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\nprint(classification_report(Y_test, prediction_dt))\n\n\n[[17 31]\n [20 86]]\n              precision    recall  f1-score   support\n\n           0       0.46      0.35      0.40        48\n           1       0.74      0.81      0.77       106\n\n    accuracy                           0.67       154\n   macro avg       0.60      0.58      0.59       154\nweighted avg       0.65      0.67      0.66       154\n\n\n\n\n\n\n\n\nResults for the Load dataset by comparing both the K Neighbors and Decision Tree Classifiers\n1. KNN: KNN gve the accuracy of 65.58%.\n2. Decision Tree: Decision tree gave the accuracy of 67.53%."
  },
  {
    "objectID": "posts/Clustering/index.html#application-of-clustering-in-machine-learning",
    "href": "posts/Clustering/index.html#application-of-clustering-in-machine-learning",
    "title": "Clustering",
    "section": "Application of Clustering in Machine Learning",
    "text": "Application of Clustering in Machine Learning\n\n1. K-Means Clustering\nK-Means is a partitioning clustering algorithm that divides data into K clusters. It works by minimizing the sum of squared distances between data points and their respective cluster centers.\n\n1.1 How does the K-NN work (detailed step)?\nThe K-NN working can be explained on the basis of the below algorithm:\n\n\nStep-1: Select the number K of the neighbors\nStep-2: Calculate the Euclidean distance of K number of neighbors\nStep-3: Take the K nearest neighbors as per the calculated Euclidean distance.\nStep-4: Among these k neighbors, count the number of the data points in each category.\nStep-5: Assign the new data points to that category for which the number of the neighbor is maximum.\nStep-6: Our model is ready.\n\n\n\n1.2 K-NN Implementation on Random Created Dataset\n\n\nCode\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for demonstration\nnp.random.seed(0)\nX = np.random.rand(100, 2)\n\n# Fit K-Means clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\n# Visualize data points and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100)\nplt.title('K-Means Clustering')\nplt.show()\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nThis visualization demonstrates K-Means clustering in action, with data points grouped into three clusters, and cluster centers shown in red.\n\n\n\n2. Hierarchical Clustering\nHierarchical clustering builds a tree-like hierarchy of clusters. It can be represented as a dendrogram, which shows the relationships between data points and clusters at different levels.\n\n2.1 How does the Hierarchical Clustering work (detailed step)?\nHierarchical clustering utilizes a metric of distance or similarity to form new clusters. The steps for Agglomerative clustering can be succinctly outlined as follows:\n\nStep-1: Compute the proximity matrix using a particular distance metric\nStep-2: Each data point is assigned to a cluster\nStep-3: Merge the clusters based on a metric for the similarity between clusters\nStep-4: Update the distance matrix\nStep-5: Repeat Step 3 and Step 4 until only a single cluster remains\n\n\n\n2.2 Hierarchical Clustering Implementation on Random Created Dataset\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Generate sample data for hierarchical clustering\ndata = np.array([[1, 2], [2, 3], [8, 8], [10, 10]])\n\n# Compute linkage matrix\nZ = linkage(data, 'single')\n\n# Create a dendrogram\nplt.figure(figsize=(10, 5))\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.show()\n\n\n\n\n\nThis dendrogram visualizes the hierarchical clustering of sample data, showing the relationships between data points and clusters.\n\n\n\n3. DBSCAN Clustering\n\n3.1 How does DBSCAN work (detailed step)?\n\nStep-1: The algorithm proceeds by arbitrarily picking up a point in the dataset (until all points have been visited).\nStep-2: If there are at least ‘minPoint’ points within a radius of ‘\\(ε\\)’ to the point then we consider all these points to be part of the same cluster.\nStep-3: The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point\n\n\n\n3.2 DBSCAN Implementation on Random Created Dataset\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\n\n# Generate synthetic data (you can replace this with your own dataset)\nX, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# Apply DBSCAN clustering\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\n# Plot the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='k', s=40)\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\n\nWe use the make_moons function from scikit-learn to generate synthetic data with two crescent moon-shaped clusters.\nDBSCAN is applied with specified parameters (eps for the maximum distance between two samples and min_samples for the number of samples in a neighborhood for a point to be considered as a core point).\nThe resulting clusters are visualized using a scatter plot where points belonging to the same cluster share the same color."
  },
  {
    "objectID": "posts/Classification/index.html#application-of-classification-in-machine-learning",
    "href": "posts/Classification/index.html#application-of-classification-in-machine-learning",
    "title": "Classification",
    "section": "Application of Classification in Machine Learning",
    "text": "Application of Classification in Machine Learning\nVarious classification methods can be employed depending on the characteristics of the dataset under consideration. This variability stems from the extensive nature of the study of classification in statistics. Below lists the common four machine learning algorithms."
  },
  {
    "objectID": "posts/Classification/index.html#python-libraries",
    "href": "posts/Classification/index.html#python-libraries",
    "title": "Classification",
    "section": "Python Libraries",
    "text": "Python Libraries\nBefore we dive into classification algorithms, we need to set up our environment. We’ll be using the following (main) Python libraries:\n\nscikit-learn: This library provides a wide range of tools for building machine learning models.\nmatplotlib and seaborn: These libraries will help us create visualizations."
  },
  {
    "objectID": "posts/Classification/index.html#dataset-selection",
    "href": "posts/Classification/index.html#dataset-selection",
    "title": "Classification",
    "section": "Dataset Selection",
    "text": "Dataset Selection\nFor this demonstration, we will use the famous Iris dataset, which contains samples of three different species of iris flowers. The goal is to classify each sample into one of these species.\n\n\nCode\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\nX = data.data\ny = data.target"
  },
  {
    "objectID": "posts/Classification/index.html#classification-algorithm",
    "href": "posts/Classification/index.html#classification-algorithm",
    "title": "Classification",
    "section": "Classification Algorithm",
    "text": "Classification Algorithm\n\n1. Naive Bayes\nAs a popular supervised machine learning algorithm, Naïve Bayes classifier is used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This modeling relies on the assumption that, given the class, the features of the input data are conditionally independent, facilitating swift and accurate predictions.\nIn statistics, Naïve Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes’ theorem. This theorem is based on the probability of a hypothesis, given the data. The Naïve Bayes classifier makes the simplifying assumption that all features in the input data are independent, a condition not always met in practical scenarios. Nevertheless, despite this simplification, the naive Bayes classifier is extensively employed due to its efficiency and commendable performance across various real-world applications.\n\n1.1 Mathematical Formulation\nBayes theorem provides a way of computing posterior probability \\(P(c|x)\\) from \\(P(c)\\), \\(P(x)\\) and \\(P(x|c)\\). Look at the equation below\n\\[\nP(c | x) = \\frac{P(x | c) P(c)}{P(x)}\n\\]\nWhere,\n\n\\(P(c|x)\\) is the posterior probability of class (c, target) given predictor (x, attributes).\n\\(P(c)\\) is the prior probability of class.\n\\(P(x|c)\\) is the likelihood which is the probability of the predictor given class.\n\\(P(x)\\) is the prior probability of the predictor.\n\n\n\n\n2. Logistic Regression\n\n\n3. K-Nearest Neighbors (KNN)\n\n\n4. Support Vector Machine\n\n\n5. Confusion Matrix"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#conclusion",
    "href": "posts/Linear and Nonlinear Regression/index.html#conclusion",
    "title": "Linear & Nonlinear Regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored both linear and nonlinear regression in machine learning. We discussed the mathematical formulas, provided Python implementations, and visualized the results using synthetic data. Linear regression is suitable for modeling linear relationships, while nonlinear regression, as demonstrated through polynomial regression, can capture more complex patterns in the data.\nUnderstanding these regression techniques is essential for modeling and predicting relationships in various fields, including economics, biology, and engineering."
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-anomalyoutlier-matrix",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-anomalyoutlier-matrix",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Anomaly/Outlier Matrix?",
    "text": "What is Anomaly/Outlier Matrix?"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-anomalyoutlier-detection",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-anomalyoutlier-detection",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Anomaly/Outlier Detection?",
    "text": "What is Anomaly/Outlier Detection?"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-anomaly-detection",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-anomaly-detection",
    "title": "Anomaly/Outlier Detection",
    "section": "What is Anomaly Detection?",
    "text": "What is Anomaly Detection?\nAnomaly detection, also known as outlier detection, refers to the process of identifying patterns or instances that deviate significantly from the norm in a dataset. Anomalies are observations that do not conform to expected behavior and may indicate unusual events, errors, or outliers in the data. Anomaly detection is applied in various fields such as finance, cybersecurity, manufacturing, and healthcare.\nThere are several approaches and algorithms used for anomaly detection:\n\nStatistical Methods:\n\nZ-Score: This method measures the number of standard deviations a data point is from the mean. Points with a high Z-score are considered anomalies.\nIQR (Interquartile Range): This method defines a range based on the interquartile range and flags points outside this range as anomalies.\n\nMachine Learning Algorithms:\n\nIsolation Forest: This algorithm builds an ensemble of isolation trees to isolate anomalies. It works by randomly selecting a feature and then randomly selecting a split value between the minimum and maximum values of the selected feature.\nOne-Class SVM (Support Vector Machine): This algorithm is trained on normal instances and aims to find a hyperplane that separates the normal instances from the outliers.\nLocal Outlier Factor (LOF): is an anomaly detection algorithm that assesses the local density deviation of data points in a dataset. LOF works on the principle that anomalies are often characterized by having a significantly lower local density compared to their neighbors.\nAutoencoders: These are neural network models that learn to encode input data into a lower-dimensional representation and then decode it back to the original data. Anomalies may have higher reconstruction errors.\n\nClustering Methods:\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): This algorithm groups together data points that are close to each other and identifies points that are in sparser regions as anomalies.\nK-Means Clustering: Anomalies may be detected by looking at instances that do not belong to any cluster or are in small clusters.\n\n\nand so on…"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#comparing-anomaly-detection-algorithms-for-outlier-detection-on-iris-datasets",
    "href": "posts/Anomaly Outlier Matrix/index.html#comparing-anomaly-detection-algorithms-for-outlier-detection-on-iris-datasets",
    "title": "Anomaly/Outlier Matrix",
    "section": "Comparing Anomaly Detection algorithms for outlier detection on Iris datasets",
    "text": "Comparing Anomaly Detection algorithms for outlier detection on Iris datasets\nBelow displays the top five rows of the iris dataset from the Scikit Learn\n\n\nCode\nfrom sklearn.datasets import load_iris \ndf = load_iris(as_frame=True).frame \ndf.head(5)\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.neighbors import LocalOutlierFactor \n\n# Load the datasets \ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the number of neighbors \nlof = LocalOutlierFactor(n_neighbors=5) \n\n# Fit the model to the data \nlof.fit(X) \n\n# Calculate the outlier scores for each point \nscores = lof.negative_outlier_factor_ \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &gt; np.percentile(scores, 95)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Local Outlier Factor',fontsize=16)      \nplt.show() \n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.ensemble import IsolationForest \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = IsolationForest(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Isolation Forest',fontsize=16)      \nplt.show()\n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.covariance import EllipticEnvelope \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = EllipticEnvelope(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Elliptic Envelope',fontsize=16)         \nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\n\n# Generate the data\nX, y = make_blobs(n_samples=1000, centers=1, cluster_std=4, random_state=123)\n\n# Define the DBSCAN parameters\neps = 3\nmin_samples = 5\n\n# Create the DBSCAN model\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\n\n# Fit the model to the data\ndbscan.fit(X)\n\n# Get the labels of the data points\nlabels = dbscan.labels_\n\n# Identify the outliers\noutliers = np.where(labels == -1)[0]\n\n# Print the number of outliers\nprint(\"Number of outliers:\", len(outliers))\n\n# Plot the data with the outliers highlighted\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.scatter(X[outliers, 0], X[outliers, 1], c=\"red\", marker=\"x\")\nplt.show()\n\n\nNumber of outliers: 1"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#comparing-anomaly-detection-algorithms-for-outlier-detection-on-iris-dataset",
    "href": "posts/Anomaly Outlier Matrix/index.html#comparing-anomaly-detection-algorithms-for-outlier-detection-on-iris-dataset",
    "title": "Anomaly/Outlier Detection",
    "section": "Comparing Anomaly Detection algorithms for outlier detection on Iris dataset",
    "text": "Comparing Anomaly Detection algorithms for outlier detection on Iris dataset\nBelow displays the top five rows of the iris dataset from the Scikit Learn which containing the following features:\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\nCode\nfrom sklearn.datasets import load_iris \ndf = load_iris(as_frame=True).frame \ndf.head(5)\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\nNext, the following provides various visualization examples using different anomaly detection algorithms\n\n1. Local Outlier Factor (LOF)\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.neighbors import LocalOutlierFactor \n\n# Load the datasets \ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the number of neighbors \nlof = LocalOutlierFactor(n_neighbors=5) \n\n# Fit the model to the data \nlof.fit(X) \n\n# Calculate the outlier scores for each point \nscores = lof.negative_outlier_factor_ \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &gt; np.percentile(scores, 95)) \n\n# Plot anomly \ncolors=['maroon','orange'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Local Outlier Factor',fontsize=16)      \nplt.show() \n\n\n\n\n\n\n\n2. Isolation Forest\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.ensemble import IsolationForest \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = IsolationForest(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['maroon','orange'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Isolation Forest',fontsize=16)      \nplt.show()\n\n\n\n\n\n\n\n3. One-class Support Vector Machines (SVMs):\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn import svm \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the nu parameter \nmodel = svm.OneClassSVM(nu=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['maroon','orange'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by One-class Support Vector Machines',fontsize=16)         \nplt.show()"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#other-example-dbscan-for-outlier-detection-and-marking-outliers",
    "href": "posts/Anomaly Outlier Matrix/index.html#other-example-dbscan-for-outlier-detection-and-marking-outliers",
    "title": "Anomaly/Outlier Detection",
    "section": "Other Example: DBSCAN for Outlier Detection and Marking Outliers",
    "text": "Other Example: DBSCAN for Outlier Detection and Marking Outliers\nLet’s explore another example of using DBSCAN to label outliers and we will also mark them with Matplotlib in a scatter plot:\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\n\n# Generate the data\nX, y = make_blobs(n_samples=1000, centers=1, cluster_std=4, random_state=123)\n\n# Define the DBSCAN parameters\neps = 3\nmin_samples = 5\n\n# Create the DBSCAN model\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\n\n# Fit the model to the data\ndbscan.fit(X)\n\n# Get the labels of the data points\nlabels = dbscan.labels_\n\n# Identify the outliers\noutliers = np.where(labels == -1)[0]\n\n# Print the number of outliers\nprint(\"Number of outliers:\", len(outliers))\n\n# Plot the data with the outliers highlighted\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.scatter(X[outliers, 0], X[outliers, 1], c=\"red\", marker=\"x\")\nplt.show()\n\n\nNumber of outliers: 1"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#example-application-of-probability-theory-in-machine-learning---probability-calibration-curves",
    "href": "posts/Probability Theory and Random Variables/index.html#example-application-of-probability-theory-in-machine-learning---probability-calibration-curves",
    "title": "Probability Theory and Random Variables",
    "section": "Example: Application of Probability Theory in Machine Learning - Probability Calibration Curves",
    "text": "Example: Application of Probability Theory in Machine Learning - Probability Calibration Curves\n\nData\nWe’ll need some data by making random values. For the purposes of this notebook we’ll use the following synthetic, two-class classification dataset, generated by the make_classification method packaged into sklearn:\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=2, n_informative=2, n_redundant=0)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nimport matplotlib.pyplot as plt\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\nplt.axis('off')\n\n\n(-4.058631209647063, 4.117395258804836, -4.309480492130218, 5.167561533094499)\n\n\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n# For binary classification tasks predict_proba returns a matrix containing the first class proba in the first entry,\n# and the second class proba in the second entry. Since there are only two classes one is just 1 - n of the other.\n# The calibration_curve implementation expects just one of these classes in an array, so we index that.\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\n\nfrom sklearn.calibration import calibration_curve\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\n\n\n\nCode\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$LogisticRegression$ Calibration Curve\", fontsize=20); pass\n\n\n\n\n\n\n\nCode\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass\n\n\n\n\n\n\n\nProbability calibration\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Create the example dataset and split it.\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Create an uncorrected classifier.\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Uncalibrated')\n\n# Create a corrected classifier.\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Calibrated (Platt)')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$RandomForestClassifier$ Sample Calibration Curve\", fontsize=20); pass\n\n\n\n\n\n\n\nCode\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Uncalibrated\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Uncalibrated')\n\n# Calibrated\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='isotonic')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Calibrated (Isotonic)')\n\n# Calibrated, Platt\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='orange', label='Calibrated (Platt)')\n\n\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#application-of-probability-theory-in-machine-learning",
    "href": "posts/Probability Theory and Random Variables/index.html#application-of-probability-theory-in-machine-learning",
    "title": "Probability Theory and Random Variables",
    "section": "3. Application of Probability Theory in Machine Learning",
    "text": "3. Application of Probability Theory in Machine Learning\n\n3.1 Data\nWe’ll need some data by making random values. For the purposes of this notebook we’ll use the following synthetic, two-class classification dataset, generated by the make_classification method packaged into sklearn:\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=2, n_informative=2, n_redundant=0)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nimport matplotlib.pyplot as plt\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='plasma')\nplt.axis('off')\n\n\n(-4.058631209647063, 4.117395258804836, -4.309480492130218, 5.167561533094499)\n\n\n\n\n\n\n\n3.2 Calibration Curves\nCalibration curves, also referred to as reliability diagrams, compare how well the probabilistic predictions of a binary classifier are calibrated. It plots the frequency of the positive label (to be more precise, an estimation of the conditional event probability \\(P(Y=1|predict\\_proba)\\) on the y-axis against the predicted probability predict_proba of a model on the x-axis.\nAn effective method for evaluating the performance of a classifier’s probability predictions on your specific dataset is through the use of a calibration curve. The procedure for constructing a calibration curve is outlined as follows:\nThe calibration curve functions by organizing the probabilities assigned to the predicted records based on the probabilities reported by the classifier. Subsequently, it categorizes these values into bins and computes two metrics.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n# For binary classification tasks predict_proba returns a matrix containing the first class proba in the first entry,\n# and the second class proba in the second entry. Since there are only two classes one is just 1 - n of the other.\n# The calibration_curve implementation expects just one of these classes in an array, so we index that.\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\n\nfrom sklearn.calibration import calibration_curve\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\n\n\n\nCode\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='green')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$LogisticRegression$ Calibration Curve\", fontsize=20); pass\n\n\n\n\n\nIn this observation, it’s evident that LogisticRegression produces probability predictions that closely align with the optimal values. While part of this alignment can be attributed to the simplicity of the dataset, the primary factor is the inherent characteristics of logistic regression. LogisticRegression is known for generating highly accurate probability predictions because it optimizes log-odds, which conveniently represents class probability. To elaborate, the cost function that LogisticRegression optimizes directly incorporates probability values. Consequently, the algorithm consistently yields unbiased and accurate probability estimates.\n\n\nCode\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='green')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass\n\n\n\n\n\nHere’s an example of an algorithm which does this poorly: GaussianNB\n\n\n3.3 Probability Calibration\nThe classical, parametric approach to probability calibration is called Platt scaling. Below is the logistic regression equation! \\(A\\) and \\(B\\) are scaling parameters, to be determined at fitting time (using some kind of maximum likelihood estimation algorithm), which control how the scaling is applied. They are calculated by applying a maximum likelihood estimation algorithm\n\\[\nP(Y=1 | x_i) = \\frac{1}{1+exp(A*f(x_i)+B)}\n\\]\nwhere:\n\n\\(x_i\\) is a record of interest\n\\(f(x_i)\\) is the probability assigned to the record by the classifier\n\nThe plots that follow show recipes for applying these transformations to the above data, and the result that they have on the calibration curves.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Create the example dataset and split it.\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Create an uncorrected classifier.\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Uncalibrated')\n\n# Create a corrected classifier.\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='green', label='Calibrated (Platt)')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$RandomForestClassifier$ Sample Calibration Curve\", fontsize=20); pass\n\n\n\n\n\nIn this example, the RandomForestClassifier was initially providing probability scores that were reasonably accurate, making a correction not strictly essential. Nevertheless, it’s apparent that implementing the Platt transformation potentially contributed to further mitigating bias.\n\n\nCode\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Uncalibrated\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='green', label='Uncalibrated')\n\n# Calibrated\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='isotonic')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Calibrated (Isotonic)')\n\n# Calibrated, Platt\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='orange', label='Calibrated (Platt)')\n\n\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass\n\n\n\n\n\nThis plot is more interesting.\nIn the blue we have the calibration curve for the original GaussianNB classifier.\nDisplayed in yellow is the recalibrated curve generated using a Platt correction. Surprisingly, the Platt correction, in this case, had a detrimental impact rather than being beneficial. Upon examining the original calibration curves for different algorithms, it becomes apparent that GaussianNB did not exhibit the same sigmoidal error structure as SVC and RandomForest in the dataset used for this example. Specifically, GaussianNB showed a tendency to undershoot the high-probability data by too much and undershoot the low probabilities by too little. Platt calibration is effective when biases on both sides are roughly equivalent, and unexpectedly, it did not perform well in this scenario.\n\nOn the contrary, isotonic calibration proves to be highly effective. In this case, the adjusted probabilities using isotonic calibration, indicated in red, closely align with the true mean probabilities."
  },
  {
    "objectID": "posts/Classification/index.html#python-main-libraries",
    "href": "posts/Classification/index.html#python-main-libraries",
    "title": "Classification",
    "section": "Python (Main) Libraries",
    "text": "Python (Main) Libraries\nBefore we dive into classification algorithms, we need to set up our environment. We’ll be using the following Python libraries:\n\nscikit-learn: This library provides a wide range of tools for building machine learning models.\nmatplotlib and seaborn: These libraries will help us create visualizations."
  },
  {
    "objectID": "posts/Classification/index.html#iris-dataset-visualization-in-machine-learning",
    "href": "posts/Classification/index.html#iris-dataset-visualization-in-machine-learning",
    "title": "Classification",
    "section": "Iris Dataset Visualization in Machine Learning",
    "text": "Iris Dataset Visualization in Machine Learning\n\n1. Python Libraries\nBefore we dive into classification algorithms, we need to set up our environment. We’ll be using the following (main) Python libraries:\n\nscikit-learn: This library provides a wide range of tools for building machine learning models.\nmatplotlib and seaborn: These libraries will help us create visualizations.\n\n\n\n2. Dataset\nFor this demonstration, we will use the famous Iris dataset, which contains samples of three different species of iris flowers. The goal is to classify each sample into one of these species.\n\n\nCode\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\nX = data.data\ny = data.target\n\n\n\n\n3. Visualizing the Data\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=data.target_names[y])\nplt.xlabel(data.feature_names[0])\nplt.ylabel(data.feature_names[1])\nplt.title(\"Iris Dataset: Sepal Length vs Sepal Width\")\nplt.show()\n\n\n\n\n\nThis plot shows the separation of iris flowers based on their sepal length and sepal width."
  }
]