[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Huayu Liang, a Master’s student in Computer Science focusing specifically on Software Engineering at Virginia Tech. I’m currently working with Dr.Chris Brown in the Code World, No Blanket lab, and I’ve been a GTA for the CS1054 - Intro to Java and am currently TA for the CS5764 - Information Visualization.\n\nFun facts about me: In middle school, I came to the United States for a summer camp as an international student, and that was a great experience in my life. I decided to study abroad in the U.S. for an undergraduate degree to broaden my horizons and study Western culture. I like doing Yoga while free and I love traveling and extreme sports such as bungee jumping, skydiving, and so on. I’d like to make friends and I am a person that is easy to go along with. Besides, I quit sugar for a couple of years to keep healthy for my diet and skin :)\n\n\n Back to top"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html",
    "href": "posts/Probability Theory and Random Variables/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Image from the source: Analytics Yogi: Maximum Likelihood Estimation: Concepts, Examples"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html",
    "href": "posts/Anomaly Outlier Matrix/index.html",
    "title": "Anomaly/Outlier Matrix",
    "section": "",
    "text": "Image from the source: Outlier Detection and Anomaly Detection with Machine Learning"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Image from the source: Analytics Yogi: K-Nearest Neighbors (KNN) Python Examples"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Here is some sample R code\n1 + 1\nBut we also want to run Python\n\n\nCode\n```{python}\nfor i in range(9):\n  print(i)\n```\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nCode\n```{python}\nx = 5\n```\n\n\n\n\nCode\n```{python}\n#| output: false\n# Testing a change in Jupyter notebook\n# I'm writing this now in RStudio\n# I'm writing this now in Jupyterlab again\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n```\n\n\nThe plot below is from the Seaborn Python library documentation\n\n\nCode\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"ticks\")\n# Initialize the figure with a logarithmic x axis\nf, ax = plt.subplots(figsize=(7, 6))\nax.set_xscale(\"log\")\n# Load the example planets dataset\nplanets = sns.load_dataset(\"planets\")\n# Plot the orbital period with horizontal boxes\nsns.boxplot(\n    planets, x=\"distance\", y=\"method\", hue=\"method\",\n    whis=[0, 100], width=.6, palette=\"vlag\"\n)\n# Add in points to show each observation\nsns.stripplot(planets, x=\"distance\", y=\"method\", size=4, color=\".3\")\n# Tweak the visual presentation\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\nsns.despine(trim=True, left=True)\n```\n\n\n\n\n\nThe plot below is from the Yellowbrick Python library documentation:\n\n\nCode\n```{python}\nimport numpy as np\nfrom yellowbrick.datasets import load_concrete\nfrom yellowbrick.features import JointPlotVisualizer\n\n# Load the dataset\nX, y = load_concrete()\n\n# Instantiate the visualizer\nvisualizer = JointPlotVisualizer(columns=\"cement\")\n\nvisualizer.fit_transform(X, y)        # Fit and transform the data\nvisualizer.show()                     # Finalize and render the figure\n```\n\n\n\n\n\n&lt;Axes: xlabel='cement', ylabel='target'&gt;\n\n\nThis is a change to the markdown text.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear & Nonlinear Regression",
    "section": "",
    "text": "Image from the source: Analytics Yogi: Linear Regression Python Examples"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Image from the source: Analytics Yogi: When to Use Which Clustering Algorithms?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Huayu Liang",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier Matrix\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nmatrix\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nLinear & Nonlinear Regression\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nlinear regressson\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nprobability theory\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nML\n\n\nvisualization\n\n\nclustering\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Machine Learning\n\n\n\n\n\n\n\nML\n\n\nintroduction\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nHuayu Liang\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/Classification/index.html#what-is-classification",
    "href": "posts/Classification/index.html#what-is-classification",
    "title": "Classification",
    "section": "What is Classification?",
    "text": "What is Classification?\nThe Classification algorithm, as a Supervised Learning technique, is employed to categorize new observations based on the knowledge gained from training data. In the classification process, the program utilizes a provided dataset or observations to learn how to assign new observations to distinct classes or groups, such as 0 or 1, red or blue, yes or no, spam or not spam, and so on. Terms like targets, labels, or categories are used interchangeably to denote these classes. As a supervised learning technique, the Classification algorithm requires labeled input data, encompassing both input and output information. The classification process involves transferring a discrete output function \\(f(y)\\) to an input variable \\((x)\\).\nIn simpler terms, classification serves as a form of pattern recognition, wherein classification algorithms analyze training data to identify similar patterns in new datasets."
  },
  {
    "objectID": "posts/Classification/index.html#dateset-selection",
    "href": "posts/Classification/index.html#dateset-selection",
    "title": "Classification",
    "section": "Dateset Selection",
    "text": "Dateset Selection"
  },
  {
    "objectID": "posts/Classification/index.html#visualizing-the-data",
    "href": "posts/Classification/index.html#visualizing-the-data",
    "title": "Classification",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=data.target_names[y])\nplt.xlabel(data.feature_names[0])\nplt.ylabel(data.feature_names[1])\nplt.title(\"Iris Dataset: Sepal Length vs Sepal Width\")\nplt.show()\n\n\n\n\n\nThis plot shows the separation of iris flowers based on their sepal length and sepal width."
  },
  {
    "objectID": "posts/Classification/index.html#classification-algorithms",
    "href": "posts/Classification/index.html#classification-algorithms",
    "title": "Classification",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\n1. Naive Bayes\nAs a popular supervised machine learning algorithm, Naïve Bayes classifier is used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This modeling relies on the assumption that, given the class, the features of the input data are conditionally independent, facilitating swift and accurate predictions.\nIn statistics, Naïve Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes’ theorem. This theorem is based on the probability of a hypothesis, given the data. The Naïve Bayes classifier makes the simplifying assumption that all features in the input data are independent, a condition not always met in practical scenarios. Nevertheless, despite this simplification, the naive Bayes classifier is extensively employed due to its efficiency and commendable performance across various real-world applications.\n\n1.1 Mathematical Formulation\nBayes theorem provides a way of computing posterior probability \\(P(c|x)\\) from \\(P(c)\\), \\(P(x)\\) and \\(P(x|c)\\). Look at the equation below\n\\[\nP(c | x) = \\frac{P(x | c) P(c)}{P(x)}\n\\]\nWhere,\n\n\\(P(c|x)\\) is the posterior probability of class (c, target) given predictor (x, attributes).\n\\(P(c)\\) is the prior probability of class.\n\\(P(x|c)\\) is the likelihood which is the probability of the predictor given class.\n\\(P(x)\\) is the prior probability of the predictor.\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Gaussian Naive Bayes classifier\nnaive_bayes_classifier = GaussianNB()\n\n# Train the classifier\nnaive_bayes_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = naive_bayes_classifier.predict(X_test)\n\n# Evaluate the classifier's performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\n# Display results\nprint(f'Accuracy: {accuracy:.2f}\\n')\nprint('Confusion Matrix:')\nprint(conf_matrix)\nprint('\\nClassification Report:')\nprint(classification_rep)\n\n# Visualize the decision boundary (2D projection for simplicity)\nplt.figure(figsize=(8, 6))\n\n# Plot training points\nfor i, c in zip(range(3), ['red', 'green', 'blue']):\n    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=c, label=f'Class {i}', edgecolors='k')\n\n# Plot testing points\nfor i, c in zip(range(3), ['red', 'green', 'blue']):\n    plt.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], c=c, marker='x', s=150, linewidth=2)\n\nplt.title('Naive Bayes Classifier - Iris Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\nAccuracy: 1.00\n\nConfusion Matrix:\n[[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      1.00      1.00         9\n           2       1.00      1.00      1.00        11\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\n\n\n\n\n\n2. Logistic Regression\n\n\nCode\n#importing libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndf= pd.read_csv(\"Iris.csv\")\ndf.drop(\"Id\",axis=1,inplace=True)    #droping id\ndf.head(5)\n\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\n\nCode\nsns.FacetGrid(df, hue=\"Species\", height=5).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\").add_legend()\n\n\n\n\n\n\n\nCode\n#let Create a pair plot of some columns \nsns.pairplot(df.iloc[:,:],hue='Species')  # graph also  tell us about the the realationship between the two columns\n\n\n\n\n\n\n\n3. K-Nearest Neighbors (KNN)\n\n\n4. Support Vector Machine\n\n\n5. Confusion Matrix"
  },
  {
    "objectID": "posts/Classification/index.html#conclusion",
    "href": "posts/Classification/index.html#conclusion",
    "title": "Classification",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Classification/classification.html",
    "href": "posts/Classification/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Image from the source: Analytics Yogi: K-Nearest Neighbors (KNN) Python Examples"
  },
  {
    "objectID": "posts/Classification/classification.html#what-is-classification",
    "href": "posts/Classification/classification.html#what-is-classification",
    "title": "Classification",
    "section": "What is Classification?",
    "text": "What is Classification?"
  },
  {
    "objectID": "posts/Classification/classification.html#dateset-selection",
    "href": "posts/Classification/classification.html#dateset-selection",
    "title": "Classification",
    "section": "Dateset Selection",
    "text": "Dateset Selection"
  },
  {
    "objectID": "posts/Classification/classification.html#visualizing-the-data",
    "href": "posts/Classification/classification.html#visualizing-the-data",
    "title": "Classification",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data"
  },
  {
    "objectID": "posts/Classification/classification.html#classification-algorithms",
    "href": "posts/Classification/classification.html#classification-algorithms",
    "title": "Classification",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\n1. Naive Bayes\n\n\n2. Logistic Regression\n\n\n3. K-Nearest Neighbors (KNN)\n\n\n4. Confusion Matrix"
  },
  {
    "objectID": "posts/Classification/classification.html#conclusion",
    "href": "posts/Classification/classification.html#conclusion",
    "title": "Classification",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Intro To Machine Learning/index.html",
    "href": "posts/Intro To Machine Learning/index.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Welcome to the Machine Learning World! This blog post covers all the basic concepts of Machine Learning that you’ll need to know as a beginner.\n\nIn today’s world, Machine Learning applications are omnipresent, seamlessly integrating into various aspects of our daily lives. Consider, for instance, the personalized search experience on Google, where Machine Learning algorithms suggest the most relevant results based on your input keywords. Similarly, major platforms like Facebook, YouTube, and Amazon employ recommendation systems to propose products tailored to individual user preferences. Notably, Apple utilizes Machine Learning algorithms for facial and fingerprint recognition, enabling users to unlock their devices without relying on traditional passwords. In essence, Machine Learning has significantly enhanced the convenience and efficiency of our everyday activities.\n\nWhat is Machine Learning?\nIn 1959, Arthur Samuel provided a defining perspective: “Machine Learning is the field of study that gives the computer the ability to learn without explicitly programmed.” A more technical definition emerged in 1997 from Tome Michel: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.”\nIn general terms, Machine Learning (ML) stands as a subset of Artificial Intelligence (AI) with a primary focus on crafting algorithms and models. These models empower computers to learn autonomously and make predictions or decisions without explicit programming. The overarching goal of machine learning is to develop systems capable of learning from data, enhancing their performance over time.\n\n\nWhy Machine Learning?\nTo underscore the significance of Machine Learning, let’s examine the case of spam email recognition. Initially, one must scrutinize the characteristics of spam emails, such as errors, links, and specific keywords like “bank card,” “free,” and “congratulation.” The traditional approach involves analyzing these traits and crafting a program to detect them consistently across all emails. This results in lengthy and intricate code that requires considerable effort for maintenance.\nIn contrast, Machine Learning automates the exploration of spam email characteristics, resulting in significantly shorter and more manageable code. Moreover, machine learning algorithms consistently outperform traditional methods.\nIn summary, several reasons advocate for the adoption of Machine Learning algorithms over traditional methods:\n\nEfficiency: Machine learning algorithms are faster, demand less computational power, and deliver superior results compared to their traditional counterparts.\nProblem-solving Capability: Machine learning techniques can tackle complex problems that traditional methods may find challenging.\nAdaptability: Machine learning systems exhibit adaptability to new data, particularly valuable in fluctuating environments.\nExploration of Complicated Problems: Machine learning algorithms excel in exploring intricate problems with large datasets.\n\nIt’s crucial to note that building a Machine Learning model requires a substantial volume of training data for accurate learning. The model often requires thousands of diverse data points to achieve high accuracy, emphasizing the paramount importance of data in the realm of Machine Learning and Data Science.\n\n\nTypes of Machine Learning Algorithms\nMachine learning algorithms can be broadly classified into three primary types: supervised learning, unsupervised learning, and reinforcement learning. Let’s delve into each category:\n\nSupervised Learning:\nDefinition: In supervised learning, the algorithm undergoes training using a labeled dataset where each input corresponds to a specific output. The objective is for the algorithm to discern the relationship between inputs and outputs, enabling it to make predictions on new, unseen data.\nExamples:\n\nClassification: Predicting discrete labels or categories (e.g., spam or not spam).\n\nRegression: Predicting continuous values (e.g., forecasting house prices).\n\n\n\n\nUnsupervised Learning:\nDefinition: Unsupervised learning entails training the algorithm on an unlabeled dataset. The algorithm’s aim is to discover patterns, structures, or relationships within the data without explicit guidance on what to identify.\n\nExamples:\n\nClustering: Grouping similar data points together (e.g., customer segmentation).\nDimensionality Reduction: Streamlining the number of features while retaining crucial information.\n\n\n\nReinforcement Learning:\nDefinition: Reinforcement learning involves an agent interacting with an environment. The agent takes actions, and the environment responds with feedback in the form of rewards or penalties. The ultimate goal is for the agent to learn optimal strategies that maximize cumulative rewards over time.\nExamples:\n\nGame Playing: Acquiring skills in playing games through rewards for successful moves.\nRobotics: Training robots to execute tasks based on feedback derived from their actions.\n\n\n\n\nUse Cases of Machine Learning\nMachine learning finds practical applications across a wide array of industries, spanning manufacturing, retail, healthcare, life sciences, travel and hospitality, financial services, and energy, feedstock, and utilities. Examples of these applications include:\n\nManufacturing: Utilizing machine learning for predictive maintenance and condition monitoring.\nRetail: Implementing machine learning for upselling and cross-channel marketing strategies.\nHealthcare and Life Sciences: Applying machine learning for disease identification and risk assessment.\nTravel and Hospitality: Employing machine learning for dynamic pricing mechanisms.\nFinancial Services: Utilizing machine learning for risk analytics and regulatory compliance.\nEnergy, Feedstock, and Utilities: Leveraging machine learning for optimizing energy demand and supply.\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#probability-theory",
    "href": "posts/Probability Theory and Random Variables/index.html#probability-theory",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability theory is the branch of mathematics that deals with uncertainty and randomness. In machine learning, it is crucial for making decisions based on uncertain or incomplete information. Let’s dive into some key concepts.\n\nCoin Toss Simulation\nAs a simple example, let’s simulate a coin toss experiment using Python. We’ll use the random module to model the randomness of the outcome.\n\n\nCode\nimport random\n\n# Simulate a coin toss\noutcomes = ['Heads', 'Tails']\nresult = random.choice(outcomes)\nprint(f\"The coin landed on: {result}\")\n\n\nThe coin landed on: Heads\n\n\n\n\nVisualizing a Coin Toss\nTo visualize the outcome of the coin toss experiment, we can create a bar chart that shows the probabilities of getting ‘Heads’ and ‘Tails’ over multiple trials.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Simulate multiple coin tosses\ntrials = 10000\ntosses = [random.choice(outcomes) for _ in range(trials)]\n\n# Count the occurrences of 'Heads' and 'Tails'\nhead_count = tosses.count('Heads')\ntail_count = tosses.count('Tails')\n\n# Create a bar chart\nplt.bar(outcomes, [head_count, tail_count])\nplt.xlabel('Outcome')\nplt.ylabel('Frequency')\nplt.title(f'Coin Toss Simulation ({trials} Trials)')\nplt.show()\n\n\n\n\n\nThis bar chart visualizes the frequencies of ‘Heads’ and ‘Tails’ outcomes over 10000 coin toss trials."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#random-variables",
    "href": "posts/Probability Theory and Random Variables/index.html#random-variables",
    "title": "Probability Theory and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nIn probability theory, a random variable is a variable whose values depend on the outcome of a random experiment. Let’s explore a discrete random variable.\n\nDice Roll Simulation\nWe’ll simulate the roll of a fair six-sided die and visualize the probability distribution of its outcomes.\n\n\nCode\n# Simulate a dice roll\ndie_faces = [1, 2, 3, 4, 5, 6]\nresult = random.choice(die_faces)\nprint(f\"The die shows: {result}\")\n\n\nThe die shows: 2\n\n\n\n\nVisualizing a Dice Roll\n\n\nCode\n# Simulate multiple dice rolls\nrolls = [random.choice(die_faces) for _ in range(trials)]\n\n# Count the occurrences of each face\nface_counts = [rolls.count(face) for face in die_faces]\n\n# Create a bar chart\nplt.bar(die_faces, face_counts)\nplt.xlabel('Die Face')\nplt.ylabel('Frequency')\nplt.title(f'Dice Roll Simulation ({trials} Rolls)')\nplt.show()\n\n\n\n\n\nThis bar chart shows the probability distribution of a fair six-sided die’s outcomes over 1000 rolls.\n\n\nProbability Calibration curves\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=2, n_informative=2, n_redundant=0)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nimport matplotlib.pyplot as plt\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\nplt.axis('off')\n\n\n(-4.058631209647063, 4.117395258804836, -4.309480492130218, 5.167561533094499)\n\n\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n# For binary classification tasks predict_proba returns a matrix containing the first class proba in the first entry,\n# and the second class proba in the second entry. Since there are only two classes one is just 1 - n of the other.\n# The calibration_curve implementation expects just one of these classes in an array, so we index that.\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\n\nfrom sklearn.calibration import calibration_curve\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\n\n\n\nCode\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$LogisticRegression$ Calibration Curve\", fontsize=20); pass\n\n\n\n\n\n\n\nCode\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nn_train_samples = 1000\n\nX_train, y_train = X[:n_train_samples], y[:n_train_samples]\nX_test, y_test = X[n_train_samples:], y[n_train_samples:]\n\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass\n\n\n\n\n\n\n\nProbability calibration\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Create the example dataset and split it.\nnp.random.seed(42)\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Create an uncorrected classifier.\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Uncalibrated')\n\n# Create a corrected classifier.\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Calibrated (Platt)')\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$RandomForestClassifier$ Sample Calibration Curve\", fontsize=20); pass\n\n\n\n\n\n\n\nCode\nX, y = make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\nfig, ax = plt.subplots(1, figsize=(12, 6))\n\n# Uncalibrated\nclf = GaussianNB()\nclf.fit(X_train, y_train)\ny_test_predict_proba = clf.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Uncalibrated')\n\n# Calibrated\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='isotonic')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\n\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='red', label='Calibrated (Isotonic)')\n\n# Calibrated, Platt\nclf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train)\ny_test_predict_proba = clf_sigmoid.predict_proba(X_test)[:, 1]\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_test_predict_proba, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', color='orange', label='Calibrated (Platt)')\n\n\nplt.plot([0, 1], [0, 1], '--', color='gray')\n\nsns.despine(left=True, bottom=True)\nplt.gca().xaxis.set_ticks_position('none')\nplt.gca().yaxis.set_ticks_position('none')\nplt.gca().legend()\nplt.title(\"$GaussianNB$ Sample Calibration Curve\", fontsize=20); pass"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#use-case-in-machine-learning",
    "href": "posts/Probability Theory and Random Variables/index.html#use-case-in-machine-learning",
    "title": "Probability Theory and Random Variables",
    "section": "Use Case in Machine Learning",
    "text": "Use Case in Machine Learning"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#code-example",
    "href": "posts/Probability Theory and Random Variables/index.html#code-example",
    "title": "Probability Theory and Random Variables",
    "section": "Code Example",
    "text": "Code Example"
  },
  {
    "objectID": "posts/Clustering/index.html#what-is-clustering",
    "href": "posts/Clustering/index.html#what-is-clustering",
    "title": "Clustering",
    "section": "What is Clustering?",
    "text": "What is Clustering?\nIn machine learning, clustering is a technique used to group a set of data points into subsets, or clusters, based on the inherent similarities among them. The primary goal of clustering is to partition the data in such a way that points within the same group are more similar to each other than they are to points in other groups.\nThe process involves organizing data points into clusters by considering certain features or characteristics, without explicit guidance on what those features should be. Unlike supervised learning, where the algorithm is trained on labeled data with predefined categories, clustering is considered unsupervised learning because it deals with unlabeled data, seeking to uncover hidden patterns or structures."
  },
  {
    "objectID": "posts/Clustering/index.html#conclusion",
    "href": "posts/Clustering/index.html#conclusion",
    "title": "Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nClustering algorithms are essential tools for finding patterns and grouping similar data points in machine learning. In this blog post, we explored three common clustering algorithms, K-Means, Hierarchical Clustering, and DBSCAN with Python implementations and visualizations. These techniques can be applied to various domains, including customer segmentation, image analysis, and more.\nTo deepen your understanding, experiment with different datasets and explore additional clustering algorithms like Gaussian Mixture Models, and Agglomerative Clustering. Visualization plays a crucial role in grasping the concepts and results of clustering."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#conclusion",
    "href": "posts/Probability Theory and Random Variables/index.html#conclusion",
    "title": "Probability Theory and Random Variables",
    "section": "Conclusion",
    "text": "Conclusion\nProbability theory and random variables are foundational concepts in machine learning that help us deal with uncertainty and randomness. In this blog post, we explored these concepts through simple simulations of coin tosses and dice rolls in Python. Visualizations played a crucial role in understanding the probability distributions of these experiments.\nUnderstanding probability theory and random variables is essential for various machine learning algorithms, such as Bayesian networks, decision trees, and more. Experiment with different simulations and visualizations to deepen your grasp of these concepts."
  },
  {
    "objectID": "posts/Clustering/index.html#application-on-of-clustering-in-machine-learning",
    "href": "posts/Clustering/index.html#application-on-of-clustering-in-machine-learning",
    "title": "Clustering",
    "section": "Application on ### of Clustering in Machine Learning",
    "text": "Application on ### of Clustering in Machine Learning"
  },
  {
    "objectID": "posts/Clustering/index.html#application-on-dataset-of-clustering-in-machine-learning",
    "href": "posts/Clustering/index.html#application-on-dataset-of-clustering-in-machine-learning",
    "title": "Clustering",
    "section": "Application on ### Dataset of Clustering in Machine Learning",
    "text": "Application on ### Dataset of Clustering in Machine Learning\n\n\nCode\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for demonstration\nnp.random.seed(0)\nX = np.random.rand(100, 2)\n\n# Fit K-Means clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\n# Visualize data points and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100)\nplt.title('K-Means Clustering')\nplt.show()\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)"
  },
  {
    "objectID": "posts/Clustering/index.html#formula-if-needed",
    "href": "posts/Clustering/index.html#formula-if-needed",
    "title": "Clustering",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#what-is-linear-regression",
    "href": "posts/Linear and Nonlinear Regression/index.html#what-is-linear-regression",
    "title": "Linear & Nonlinear Regression",
    "section": "What is Linear Regression?",
    "text": "What is Linear Regression?\nLinear regression is a straightforward approach for modeling the relationship between a dependent variable \\((Y)\\) and one or more independent variables \\((X)\\).\n\nFormula\nThe formula for simple linear regression is:\n\\[\nY = \\beta_0 + \\beta_1 X\n\\]\nwhere:\n\n\\(Y\\) is the dependent variable.\n\\(X\\) is the independent variable.\n\\(\\beta_0\\)​ is the intercept.\n\\(\\beta_1\\)​ is the slope.\n\n\n\nPython Implementation\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Generate 'random' data\nnp.random.seed(0)\nX = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5\nres = 0.5 * np.random.randn(100)       # Generate 100 residual terms\ny = 2 + 0.3 * X + res                  # Actual values of Y\n\n# Create pandas dataframe to store our X and y values\ndf = pd.DataFrame(\n    {'X': X,\n     'y': y}\n)\n\n# Show the first five rows of our dataframe\ndf.head()\n\n\n\n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n5.910131\n4.714615\n\n\n1\n2.500393\n2.076238\n\n\n2\n3.946845\n2.548811\n\n\n3\n7.102233\n4.615368\n\n\n4\n6.168895\n3.264107\n\n\n\n\n\n\n\n\n\nCode\n# Calculate the mean of X and y\nxmean = np.mean(X)\nymean = np.mean(y)\n\n# Calculate the terms needed for the numerator and denominator of beta\ndf['xycov'] = (df['X'] - xmean) * (df['y'] - ymean)\ndf['xvar'] = (df['X'] - xmean)**2\n\n# Calculate beta and alpha\nbeta = df['xycov'].sum() / df['xvar'].sum()\nalpha = ymean - (beta * xmean)\nprint(f'alpha = {alpha}')\nprint(f'beta = {beta}')\n\n\nalpha = 2.0031670124623426\nbeta = 0.3229396867092763\n\n\n\n\nCode\nypred = alpha + beta * X\n\n# Plot regression against actual data\nplt.figure(figsize=(12, 6))\nplt.plot(X, ypred)     # regression line\nplt.plot(X, y, 'ro')   # scatter plot showing actual data\nplt.title('Actual vs Predicted')\nplt.xlabel('X')\nplt.ylabel('y')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Import and display first five rows of advertising dataset\nadvert = pd.read_csv('advertising.csv')\nadvert.head(5)\n\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.formula.api as smf\n\n# Initialise and fit linear regression model using `statsmodels`\nmodel = smf.ols('Sales ~ TV', data=advert)\nmodel = model.fit()\n\n\n# Predict values\nsales_pred = model.predict()\n\n# Plot regression against actual data\nplt.figure(figsize=(12, 6))\nplt.plot(advert['TV'], advert['Sales'], 'o')           # scatter plot showing actual data\nplt.plot(advert['TV'], sales_pred, 'r', linewidth=2)   # regression line\nplt.xlabel('TV Advertising Costs')\nplt.ylabel('Sales')\nplt.title('TV vs Sales')\n\nplt.show()\n\n\n\n\n\n\n\nCode\nnew_X = 400\nmodel.predict({\"TV\": new_X})\n\n\n0    29.16073\ndtype: float64\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\n# Build linear regression model using TV and Radio as predictors\n# Split data into predictors X and output Y\npredictors = ['TV', 'Radio']\nX = advert[predictors]\ny = advert['Sales']\n\n# Initialise and fit model\nlm = LinearRegression()\nmodel = lm.fit(X, y)\n\nprint(f'alpha = {model.intercept_}')\nprint(f'betas = {model.coef_}')\nmodel.predict(X)\n\nnew_X = [[300, 200]]\nprint(model.predict(new_X))\n\n\nalpha = 4.630879464097763\nbetas = [0.05444896 0.10717457]\n[42.40048195]\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn("
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#what-is-non-linear-regression",
    "href": "posts/Linear and Nonlinear Regression/index.html#what-is-non-linear-regression",
    "title": "Linear & Nonlinear Regression",
    "section": "What is Non-Linear Regression?",
    "text": "What is Non-Linear Regression?\nNonlinear regression is used when the relationship between variables is not linear and cannot be accurately represented by a straight line.\n\nFormula\nThe formula for a simple nonlinear regression can vary depending on the chosen model. Let’s consider a simple polynomial regression:\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2\n\\]\nwhere:\n\n\\(Y\\) is the dependent variable.\n\\(X\\) is the independent variable.\n\\(\\beta_0\\)​ is the intercept.\n\\(\\beta_1\\)​ is the coefficient for the linear term.\n\\(\\beta_2\\)​ is the coefficient for the quadratic term.\n\n\n\nPython Implementation\n\n\nCode\n# Import the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# get the dataset\ndataset = pd.read_csv('Position_Salaries.csv')\n# View the dataset\ndataset.head(5)\n\n\n\n\n\n\n\n\n\nPosition\nLevel\nSalary\n\n\n\n\n0\nBusiness Analyst\n1\n45000\n\n\n1\nJunior Consultant\n2\n50000\n\n\n2\nSenior Consultant\n3\n60000\n\n\n3\nManager\n4\n80000\n\n\n4\nCountry Manager\n5\n110000\n\n\n\n\n\n\n\n\n\nCode\n# split the data into featutes and target variable seperately\nX_l = dataset.iloc[:, 1:-1].values # features set\ny_p = dataset.iloc[:, -1].values # set of study variable\nprint('Unique Level: ', X_l)\nprint('Unique Salary: ', y_p)\ny_p = y_p.reshape(-1,1)\n\n\nUnique Level:  [[ 1]\n [ 2]\n [ 3]\n [ 4]\n [ 5]\n [ 6]\n [ 7]\n [ 8]\n [ 9]\n [10]]\nUnique Salary:  [  45000   50000   60000   80000  110000  150000  200000  300000  500000\n 1000000]\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nStdS_X = StandardScaler()\nStdS_y = StandardScaler()\nX_l = StdS_X.fit_transform(X_l)\ny_p = StdS_y.fit_transform(y_p)\n\n\n\n\nCode\nplt.scatter(X_l, y_p, color = 'red') # plotting the training set\nplt.title('Scatter Plot') # adding a tittle to our plot\nplt.xlabel('Levels') # adds a label to the x-axis\nplt.ylabel('Salary') # adds a label to the y-axis\nplt.show()\n\n\n\n\n\n\n\nCode\n# import the model\nfrom sklearn.svm import SVR\n# create the model object\nregressor = SVR(kernel = 'rbf')\n# fit the model on the data\nregressor.fit(X_l, y_p)\n# Make a prediction\nA=regressor.predict(StdS_X.transform([[6.5]]))\nprint(A)\n\n\n[-0.27861589]\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n\n\nCode\n# inverse the transformation to go back to the initial scale\nplt.scatter(StdS_X.inverse_transform(X_l), StdS_y.inverse_transform(y_p), color = 'red')\nplt.plot(StdS_X.inverse_transform(X_l), StdS_y.inverse_transform(regressor.predict(X_l).reshape(-1,1)), color = 'blue')\n# add the title to the plot\nplt.title('Support Vector Regression Model')\n# label x axis\nplt.xlabel('Position')\n# label y axis\nplt.ylabel('Salary Level')\n# print the plot\nplt.show()"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#formulas-if-needed",
    "href": "posts/Linear and Nonlinear Regression/index.html#formulas-if-needed",
    "title": "Linear & Nonlinear Regression",
    "section": "Formulas (if needed)",
    "text": "Formulas (if needed)"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#application-on-dataset-of-regression-in-machine-learning",
    "href": "posts/Linear and Nonlinear Regression/index.html#application-on-dataset-of-regression-in-machine-learning",
    "title": "Linear & Nonlinear Regression",
    "section": "Application on ### Dataset of Regression in Machine Learning",
    "text": "Application on ### Dataset of Regression in Machine Learning"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#conslusions",
    "href": "posts/Linear and Nonlinear Regression/index.html#conslusions",
    "title": "Linear & Nonlinear Regression",
    "section": "Conslusions",
    "text": "Conslusions"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#conslusion",
    "href": "posts/Linear and Nonlinear Regression/index.html#conslusion",
    "title": "Linear & Nonlinear Regression",
    "section": "Conslusion",
    "text": "Conslusion"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-anomaly-matrix",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-anomaly-matrix",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Anomaly Matrix?",
    "text": "What is Anomaly Matrix?"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-outlier-matrix",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-outlier-matrix",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Outlier Matrix?",
    "text": "What is Outlier Matrix?"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#application-on-dataset-of-anomalyoutlier-matrix-in-machine-learning",
    "href": "posts/Anomaly Outlier Matrix/index.html#application-on-dataset-of-anomalyoutlier-matrix-in-machine-learning",
    "title": "Anomaly/Outlier Matrix",
    "section": "Application on ### Dataset of Anomaly/Outlier Matrix in Machine Learning",
    "text": "Application on ### Dataset of Anomaly/Outlier Matrix in Machine Learning\n\n\nCode\nfrom sklearn.datasets import load_iris \ndf = load_iris(as_frame=True).frame \ndf.head(5)\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.neighbors import LocalOutlierFactor \n\n# Load the datasets \ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the number of neighbors \nlof = LocalOutlierFactor(n_neighbors=5) \n\n# Fit the model to the data \nlof.fit(X) \n\n# Calculate the outlier scores for each point \nscores = lof.negative_outlier_factor_ \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &gt; np.percentile(scores, 95)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Local Outlier Factor',fontsize=16)      \nplt.show() \n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.ensemble import IsolationForest \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = IsolationForest(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Isolation Forest',fontsize=16)      \nplt.show()\n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.covariance import EllipticEnvelope \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = EllipticEnvelope(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Elliptic Envelope',fontsize=16)         \nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\n\n# Generate the data\nX, y = make_blobs(n_samples=1000, centers=1, cluster_std=4, random_state=123)\n\n# Define the DBSCAN parameters\neps = 3\nmin_samples = 5\n\n# Create the DBSCAN model\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\n\n# Fit the model to the data\ndbscan.fit(X)\n\n# Get the labels of the data points\nlabels = dbscan.labels_\n\n# Identify the outliers\noutliers = np.where(labels == -1)[0]\n\n# Print the number of outliers\nprint(\"Number of outliers:\", len(outliers))\n\n# Plot the data with the outliers highlighted\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.scatter(X[outliers, 0], X[outliers, 1], c=\"red\", marker=\"x\")\nplt.show()\n\n\nNumber of outliers: 1"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#conclusion",
    "href": "posts/Anomaly Outlier Matrix/index.html#conclusion",
    "title": "Anomaly/Outlier Matrix",
    "section": "Conclusion",
    "text": "Conclusion\nAnomaly detection is a critical task in various fields, including fraud detection, network security, and quality control. You can experiment with different datasets, explore other anomaly detection algorithms like K-Means Clustering, Z-score (statistic), and Autoencoders, and adjust threshold values to adapt to specific use cases."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#formula-if-needed",
    "href": "posts/Probability Theory and Random Variables/index.html#formula-if-needed",
    "title": "Probability Theory and Random Variables",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#formula-if-needed",
    "href": "posts/Linear and Nonlinear Regression/index.html#formula-if-needed",
    "title": "Linear & Nonlinear Regression",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Classification/index.html#formula-if-needed",
    "href": "posts/Classification/index.html#formula-if-needed",
    "title": "Classification",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#formula-if-needed",
    "href": "posts/Anomaly Outlier Matrix/index.html#formula-if-needed",
    "title": "Anomaly/Outlier Matrix",
    "section": "Formula (if needed)",
    "text": "Formula (if needed)"
  },
  {
    "objectID": "posts/Classification/index.html#types-of-classification-algorithms",
    "href": "posts/Classification/index.html#types-of-classification-algorithms",
    "title": "Classification",
    "section": "Types of Classification Algorithms",
    "text": "Types of Classification Algorithms\nVarious classification methods can be employed depending on the characteristics of the dataset under consideration. This variability stems from the extensive nature of the study of classification in statistics. Below lists the top five machine learning algorithms.\n\n1. Naive Bayes\nAs a popular supervised machine learning algorithm, Naïve Bayes classifier is used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This modeling relies on the assumption that, given the class, the features of the input data are conditionally independent, facilitating swift and accurate predictions.\nIn statistics, Naïve Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes’ theorem. This theorem is based on the probability of a hypothesis, given the data. The Naïve Bayes classifier makes the simplifying assumption that all features in the input data are independent, a condition not always met in practical scenarios. Nevertheless, despite this simplification, the naive Bayes classifier is extensively employed due to its efficiency and commendable performance across various real-world applications.\n\n1.1 Mathematical Formulation\nBayes theorem provides a way of computing posterior probability \\(P(c|x)\\) from \\(P(c)\\), \\(P(x)\\) and \\(P(x|c)\\). Look at the equation below\n\\[\nP(c | x) = \\frac{P(x | c) P(c)}{P(x)}\n\\]\nWhere,\n\n\\(P(c|x)\\) is the posterior probability of class (c, target) given predictor (x, attributes).\n\\(P(c)\\) is the prior probability of class.\n\\(P(x|c)\\) is the likelihood which is the probability of the predictor given class.\n\\(P(x)\\) is the prior probability of the predictor.\n\n\n\n\n2. Logistic Regression\n\n\n3. K-Nearest Neighbors (KNN)\n\n\n4. Support Vector Machine\n\n\n5. Confusion Matrix"
  },
  {
    "objectID": "posts/Clustering/index.html#application-of-clustering-in-machine-learning",
    "href": "posts/Clustering/index.html#application-of-clustering-in-machine-learning",
    "title": "Clustering",
    "section": "Application of Clustering in Machine Learning",
    "text": "Application of Clustering in Machine Learning\n\nK-Means Clustering\nK-Means is a partitioning clustering algorithm that divides data into K clusters. It works by minimizing the sum of squared distances between data points and their respective cluster centers.\n\n\nCode\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for demonstration\nnp.random.seed(0)\nX = np.random.rand(100, 2)\n\n# Fit K-Means clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\n# Visualize data points and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100)\nplt.title('K-Means Clustering')\nplt.show()\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nThis visualization demonstrates K-Means clustering in action, with data points grouped into three clusters, and cluster centers shown in red.\n\n\nHierarchical Clustering\nHierarchical clustering builds a tree-like hierarchy of clusters. It can be represented as a dendrogram, which shows the relationships between data points and clusters at different levels.\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Generate sample data for hierarchical clustering\ndata = np.array([[1, 2], [2, 3], [8, 8], [10, 10]])\n\n# Compute linkage matrix\nZ = linkage(data, 'single')\n\n# Create a dendrogram\nplt.figure(figsize=(10, 5))\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.show()\n\n\n\n\n\nThis dendrogram visualizes the hierarchical clustering of sample data, showing the relationships between data points and clusters.\n\n\nDBSCAN labels for the Scatter Plot\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\n\n# Generate synthetic data (you can replace this with your own dataset)\nX, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# Apply DBSCAN clustering\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\n# Plot the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='k', s=40)\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\n\nWe use the make_moons function from scikit-learn to generate synthetic data with two crescent moon-shaped clusters.\nDBSCAN is applied with specified parameters (eps for the maximum distance between two samples and min_samples for the number of samples in a neighborhood for a point to be considered as a core point).\nThe resulting clusters are visualized using a scatter plot where points belonging to the same cluster share the same color."
  },
  {
    "objectID": "posts/Classification/index.html#application-of-classification-in-machine-learning",
    "href": "posts/Classification/index.html#application-of-classification-in-machine-learning",
    "title": "Classification",
    "section": "Application of Classification in Machine Learning",
    "text": "Application of Classification in Machine Learning\nVarious classification methods can be employed depending on the characteristics of the dataset under consideration. This variability stems from the extensive nature of the study of classification in statistics. Below lists the top five machine learning algorithms."
  },
  {
    "objectID": "posts/Classification/index.html#python-libraries",
    "href": "posts/Classification/index.html#python-libraries",
    "title": "Classification",
    "section": "Python Libraries",
    "text": "Python Libraries\nBefore we dive into classification algorithms, we need to set up our environment. We’ll be using the following Python libraries:\n\nscikit-learn: This library provides a wide range of tools for building machine learning models.\nmatplotlib and seaborn: These libraries will help us create visualizations."
  },
  {
    "objectID": "posts/Classification/index.html#dataset-selection",
    "href": "posts/Classification/index.html#dataset-selection",
    "title": "Classification",
    "section": "Dataset Selection",
    "text": "Dataset Selection\nFor this demonstration, we will use the famous Iris dataset, which contains samples of three different species of iris flowers. The goal is to classify each sample into one of these species.\n\n\nCode\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\nX = data.data\ny = data.target"
  },
  {
    "objectID": "posts/Classification/index.html#classification-algorithm",
    "href": "posts/Classification/index.html#classification-algorithm",
    "title": "Classification",
    "section": "Classification Algorithm",
    "text": "Classification Algorithm\n\n1. Naive Bayes\nAs a popular supervised machine learning algorithm, Naïve Bayes classifier is used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This modeling relies on the assumption that, given the class, the features of the input data are conditionally independent, facilitating swift and accurate predictions.\nIn statistics, Naïve Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes’ theorem. This theorem is based on the probability of a hypothesis, given the data. The Naïve Bayes classifier makes the simplifying assumption that all features in the input data are independent, a condition not always met in practical scenarios. Nevertheless, despite this simplification, the naive Bayes classifier is extensively employed due to its efficiency and commendable performance across various real-world applications.\n\n1.1 Mathematical Formulation\nBayes theorem provides a way of computing posterior probability \\(P(c|x)\\) from \\(P(c)\\), \\(P(x)\\) and \\(P(x|c)\\). Look at the equation below\n\\[\nP(c | x) = \\frac{P(x | c) P(c)}{P(x)}\n\\]\nWhere,\n\n\\(P(c|x)\\) is the posterior probability of class (c, target) given predictor (x, attributes).\n\\(P(c)\\) is the prior probability of class.\n\\(P(x|c)\\) is the likelihood which is the probability of the predictor given class.\n\\(P(x)\\) is the prior probability of the predictor.\n\n\n\n\n2. Logistic Regression\n\n\n3. K-Nearest Neighbors (KNN)\n\n\n4. Support Vector Machine\n\n\n5. Confusion Matrix"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#conclusion",
    "href": "posts/Linear and Nonlinear Regression/index.html#conclusion",
    "title": "Linear & Nonlinear Regression",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-anomalyoutlier-matrix",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-anomalyoutlier-matrix",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Anomaly/Outlier Matrix?",
    "text": "What is Anomaly/Outlier Matrix?"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-anomalyoutlier-detection",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-anomalyoutlier-detection",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Anomaly/Outlier Detection?",
    "text": "What is Anomaly/Outlier Detection?"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#what-is-anomaly-detection",
    "href": "posts/Anomaly Outlier Matrix/index.html#what-is-anomaly-detection",
    "title": "Anomaly/Outlier Matrix",
    "section": "What is Anomaly Detection?",
    "text": "What is Anomaly Detection?\nAnomaly detection, also known as outlier detection, refers to the process of identifying patterns or instances that deviate significantly from the norm in a dataset. Anomalies are observations that do not conform to expected behavior and may indicate unusual events, errors, or outliers in the data. Anomaly detection is applied in various fields such as finance, cybersecurity, manufacturing, and healthcare.\nThere are several approaches and algorithms used for anomaly detection:\n\nStatistical Methods:\n\nZ-Score: This method measures the number of standard deviations a data point is from the mean. Points with a high Z-score are considered anomalies.\nIQR (Interquartile Range): This method defines a range based on the interquartile range and flags points outside this range as anomalies.\n\nMachine Learning Algorithms:\n\nIsolation Forest: This algorithm builds an ensemble of isolation trees to isolate anomalies. It works by randomly selecting a feature and then randomly selecting a split value between the minimum and maximum values of the selected feature.\nOne-Class SVM (Support Vector Machine): This algorithm is trained on normal instances and aims to find a hyperplane that separates the normal instances from the outliers.\nLocal Outlier Factor (LOF): is an anomaly detection algorithm that assesses the local density deviation of data points in a dataset. LOF works on the principle that anomalies are often characterized by having a significantly lower local density compared to their neighbors.\nAutoencoders: These are neural network models that learn to encode input data into a lower-dimensional representation and then decode it back to the original data. Anomalies may have higher reconstruction errors.\n\nClustering Methods:\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): This algorithm groups together data points that are close to each other and identifies points that are in sparser regions as anomalies.\nK-Means Clustering: Anomalies may be detected by looking at instances that do not belong to any cluster or are in small clusters.\n\n\nand so on…"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#comparing-anomaly-detection-algorithms-for-outlier-detection-on-iris-datasets",
    "href": "posts/Anomaly Outlier Matrix/index.html#comparing-anomaly-detection-algorithms-for-outlier-detection-on-iris-datasets",
    "title": "Anomaly/Outlier Matrix",
    "section": "Comparing Anomaly Detection algorithms for outlier detection on Iris datasets",
    "text": "Comparing Anomaly Detection algorithms for outlier detection on Iris datasets\nBelow displays the top five rows of the iris dataset from the Scikit Learn\n\n\nCode\nfrom sklearn.datasets import load_iris \ndf = load_iris(as_frame=True).frame \ndf.head(5)\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.neighbors import LocalOutlierFactor \n\n# Load the datasets \ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the number of neighbors \nlof = LocalOutlierFactor(n_neighbors=5) \n\n# Fit the model to the data \nlof.fit(X) \n\n# Calculate the outlier scores for each point \nscores = lof.negative_outlier_factor_ \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &gt; np.percentile(scores, 95)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Local Outlier Factor',fontsize=16)      \nplt.show() \n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.ensemble import IsolationForest \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = IsolationForest(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Isolation Forest',fontsize=16)      \nplt.show()\n\n\n\n\n\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.covariance import EllipticEnvelope \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = EllipticEnvelope(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Elliptic Envelope',fontsize=16)         \nplt.show()\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\n\n# Generate the data\nX, y = make_blobs(n_samples=1000, centers=1, cluster_std=4, random_state=123)\n\n# Define the DBSCAN parameters\neps = 3\nmin_samples = 5\n\n# Create the DBSCAN model\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\n\n# Fit the model to the data\ndbscan.fit(X)\n\n# Get the labels of the data points\nlabels = dbscan.labels_\n\n# Identify the outliers\noutliers = np.where(labels == -1)[0]\n\n# Print the number of outliers\nprint(\"Number of outliers:\", len(outliers))\n\n# Plot the data with the outliers highlighted\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.scatter(X[outliers, 0], X[outliers, 1], c=\"red\", marker=\"x\")\nplt.show()\n\n\nNumber of outliers: 1"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#comparing-anomaly-detection-algorithms-for-outlier-detection-on-iris-dataset",
    "href": "posts/Anomaly Outlier Matrix/index.html#comparing-anomaly-detection-algorithms-for-outlier-detection-on-iris-dataset",
    "title": "Anomaly/Outlier Matrix",
    "section": "Comparing Anomaly Detection algorithms for outlier detection on Iris dataset",
    "text": "Comparing Anomaly Detection algorithms for outlier detection on Iris dataset\nBelow displays the top five rows of the iris dataset from the Scikit Learn which containing the following features:\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\nCode\nfrom sklearn.datasets import load_iris \ndf = load_iris(as_frame=True).frame \ndf.head(5)\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\nNext, the following provides various examples of anomaly detection algorithms\n\n1. Local Outlier Factor (LOF)\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.neighbors import LocalOutlierFactor \n\n# Load the datasets \ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the number of neighbors \nlof = LocalOutlierFactor(n_neighbors=5) \n\n# Fit the model to the data \nlof.fit(X) \n\n# Calculate the outlier scores for each point \nscores = lof.negative_outlier_factor_ \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &gt; np.percentile(scores, 95)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Local Outlier Factor',fontsize=16)      \nplt.show() \n\n\n\n\n\n\n\n2. Isolation Forest\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn.ensemble import IsolationForest \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the contamination level \nmodel = IsolationForest(contamination=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by Isolation Forest',fontsize=16)      \nplt.show()\n\n\n\n\n\n\n\n3. One-class Support Vector Machines (SVMs):\n\n\nCode\n# Import the necessary modules \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import load_iris \nfrom sklearn import svm \n\ndf = load_iris(as_frame=True).frame \nX = df[['sepal length (cm)','sepal width (cm)']] \n\n# Define the model and set the nu parameter \nmodel = svm.OneClassSVM(nu=0.05) \n\n# Fit the model to the data \nmodel.fit(X) \n\n# Calculate the outlier scores for each point \nscores = model.decision_function(X) \n\n# Identify the points with the highest outlier scores \noutliers = np.argwhere(scores &lt; np.percentile(scores, 5)) \n\n# Plot anomly \ncolors=['green','red'] \n\nfor i in range(len(X)): \n    if i not in outliers: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly \n    else: \n        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly \nplt.xlabel('sepal length (cm)',fontsize=13) \nplt.ylabel('sepal width (cm)',fontsize=13)       \nplt.title('Anomly by One-class Support Vector Machines',fontsize=16)         \nplt.show()"
  },
  {
    "objectID": "posts/Anomaly Outlier Matrix/index.html#other-example-dbscan-for-outlier-detection-and-marking-outliers",
    "href": "posts/Anomaly Outlier Matrix/index.html#other-example-dbscan-for-outlier-detection-and-marking-outliers",
    "title": "Anomaly/Outlier Matrix",
    "section": "Other Example: DBSCAN for Outlier Detection and Marking Outliers",
    "text": "Other Example: DBSCAN for Outlier Detection and Marking Outliers\nLet’s explore another example of using DBSCAN to label outliers and we will also mark them with Matplotlib in a scatter plot:\n\n\nCode\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\n\n# Generate the data\nX, y = make_blobs(n_samples=1000, centers=1, cluster_std=4, random_state=123)\n\n# Define the DBSCAN parameters\neps = 3\nmin_samples = 5\n\n# Create the DBSCAN model\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\n\n# Fit the model to the data\ndbscan.fit(X)\n\n# Get the labels of the data points\nlabels = dbscan.labels_\n\n# Identify the outliers\noutliers = np.where(labels == -1)[0]\n\n# Print the number of outliers\nprint(\"Number of outliers:\", len(outliers))\n\n# Plot the data with the outliers highlighted\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.scatter(X[outliers, 0], X[outliers, 1], c=\"red\", marker=\"x\")\nplt.show()\n\n\nNumber of outliers: 1"
  }
]